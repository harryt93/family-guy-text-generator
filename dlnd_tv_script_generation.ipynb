{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Family Guy Script Generation\n",
    "\n",
    "## Get the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import helper\n",
    "\n",
    "data_dir = './data/family-guy-20.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "Play around with `view_sentence_range` to view different parts of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 10799\n",
      "Number of scenes: 688\n",
      "Average number of sentences in each scene: 5.867732558139535\n",
      "Number of lines: 4725\n",
      "Average number of words in each line: 12.873015873015873\n",
      "\n",
      "The sentences 0 to 10:\n",
      "LOIS: Smoking. How does a boy like that go so wrong?\n",
      "PETER: They live in a crummy neighborhood.\n",
      "BRIAN: The Bradys?\n",
      "PETER: Yeah. They got robbers, thugs, drug dealers. You name it.\n",
      "GUY: You folks want some pancakes? \n",
      "PETER: No, thanks. See, that's the worst we got is Jemima's Witnesses.\n",
      "\n",
      "MEG: Mom, my lips are too thin. Can I please get collagen injections? \n",
      "LOIS: Meg, you don't need to change the way you look. You know, most of the world's problems stem from poor self-image.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_sentence_range = (0, 10)\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "scenes = text.split('\\n\\n')\n",
    "print('Number of scenes: {}'.format(len(scenes)))\n",
    "sentence_count_scene = [scene.count('\\n') for scene in scenes]\n",
    "print('Average number of sentences in each scene: {}'.format(np.average(sentence_count_scene)))\n",
    "\n",
    "sentences = [sentence for scene in scenes for sentence in scene.split('\\n')]\n",
    "print('Number of lines: {}'.format(len(sentences)))\n",
    "word_count_sentence = [len(sentence.split()) for sentence in sentences]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_sentence)))\n",
    "\n",
    "print()\n",
    "print('The sentences {} to {}:'.format(*view_sentence_range))\n",
    "print('\\n'.join(text.split('\\n')[view_sentence_range[0]:view_sentence_range[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import problem_unittests as tests\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "   \n",
    "    words = set(text)   \n",
    "    \n",
    "    int_to_vocab = {i: word for i, word in enumerate(words)}\n",
    "    \n",
    "    vocab_to_int = {word: i for i, word in enumerate(words)}\n",
    "  \n",
    "    return vocab_to_int, int_to_vocab\n",
    "\n",
    "def token_lookup():\n",
    "    \"\"\"\n",
    "    Generate a dict to turn punctuation into a token.\n",
    "    :return: Tokenize dictionary where the key is the punctuation and the value is the token\n",
    "    \"\"\"\n",
    "    punct_to_token = {'.': '<PERIOD>',\n",
    "                      ',': '<COMMA>',\n",
    "                      '\"': '<QUOTATION>',\n",
    "                      ';': '<SEMICOLON>',\n",
    "                      '!': '<EXCLAMATION>',\n",
    "                      '?': '<QUESTION>',\n",
    "                      '(': '<LEFTP>',\n",
    "                      ')': '<RIGHTP>',\n",
    "                      '[': '<LEFTB>',\n",
    "                      ']': '<RIGHTB>',\n",
    "                      '--': '<DASH>',\n",
    "                      '\\n': '<RETURN>',}\n",
    "    \n",
    "    return punct_to_token\n",
    "\n",
    "def get_tensors(loaded_graph):\n",
    "\n",
    "    InputTensor = loaded_graph.get_tensor_by_name(\"input:0\")\n",
    "    InitialStateTensor = loaded_graph.get_tensor_by_name(\"initial_state:0\")\n",
    "    FinalStateTensor = loaded_graph.get_tensor_by_name(\"final_state:0\")\n",
    "    ProbsTensor = loaded_graph.get_tensor_by_name(\"probs:0\")\n",
    "    \n",
    "    return InputTensor, InitialStateTensor, FinalStateTensor, ProbsTensor\n",
    "\n",
    "def pick_word(probabilities, int_to_vocab):\n",
    "    \n",
    "    index = np.argmax(probabilities)\n",
    "\n",
    "    word = int_to_vocab[index]\n",
    "    \n",
    "    return word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)\n",
    "\n",
    "import helper\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Neural Network\n",
    "\n",
    "### Setup Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.2.1\n",
      "Default GPU Device: /gpu:0\n"
     ]
    }
   ],
   "source": [
    "from distutils.version import LooseVersion\n",
    "import warnings\n",
    "import tensorflow as tf\n",
    "\n",
    "# Check TensorFlow Version\n",
    "assert LooseVersion(tf.__version__) >= LooseVersion('1.0'), 'Please use TensorFlow version 1.0 or newer'\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))\n",
    "\n",
    "# Check for a GPU\n",
    "if not tf.test.gpu_device_name():\n",
    "    warnings.warn('No GPU found. Please use a GPU to train your neural network.')\n",
    "else:\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_inputs():\n",
    "\n",
    "    Input = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    Targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    LearningRate = tf.placeholder(tf.float32, name='learningrate')\n",
    "\n",
    "    return Input, Targets, LearningRate\n",
    "\n",
    "def get_init_cell(batch_size, rnn_size):\n",
    "\n",
    "    # Set the size of the LSTMs\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size) \n",
    "    cell = tf.contrib.rnn.MultiRNNCell([lstm]*1)\n",
    "\n",
    "    initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "\n",
    "    initial_state = tf.identity(initial_state, name=\"initial_state\")\n",
    "\n",
    "    return cell, initial_state\n",
    "\n",
    "def get_embed(input_data, vocab_size, embed_dim):\n",
    " \n",
    "    embedding = tf.Variable(tf.random_uniform((vocab_size, embed_dim), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, input_data)\n",
    "\n",
    "    return embed\n",
    "\n",
    "def build_rnn(cell, inputs):\n",
    "\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "    \n",
    "    final_state = tf.identity(final_state, name=\"final_state\")\n",
    "        \n",
    "    return outputs, final_state\n",
    "\n",
    "def build_nn(cell, rnn_size, input_data, vocab_size, embed_dim):\n",
    " \n",
    "    embedding_layer = get_embed(input_data, vocab_size, embed_dim)\n",
    "    \n",
    "    rnn_layer, final_state = build_rnn(cell, embedding_layer)\n",
    "    \n",
    "    logits = tf.contrib.layers.fully_connected(inputs=rnn_layer, num_outputs=vocab_size, activation_fn=None)\n",
    "    \n",
    "    return logits, final_state\n",
    "\n",
    "def get_batches(int_text, batch_size, seq_length):\n",
    " \n",
    "    # Turn the input text into an np array.\n",
    "    int_text = np.array(int_text)\n",
    "    \n",
    "    # Get the number of characters per batch and number of batches we can make\n",
    "    characters_per_batch = batch_size * seq_length\n",
    "    n_batches = len(int_text)//characters_per_batch\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    int_text = int_text[:n_batches * characters_per_batch]\n",
    "    \n",
    "    # Reshape into n_seqs rows\n",
    "    int_text = int_text.reshape((batch_size, -1))\n",
    "     \n",
    "    # Create the empty list to start adding batches to.\n",
    "    batches = []\n",
    "    \n",
    "    for n in range(0, int_text.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = int_text[:, n:n+seq_length]\n",
    "        \n",
    "        # The targets, shifted by one\n",
    "        rolled = np.roll(int_text, -1)\n",
    "        y = rolled[:, n:n+seq_length]        \n",
    "        \n",
    "        z = np.array([x, y])\n",
    "        batches.append(z)\n",
    "        \n",
    "    # Turn batches into a numpy array.\n",
    "    batches = np.array(batches)        \n",
    "        \n",
    "    return batches\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Training\n",
    "### Hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of Epochs\n",
    "num_epochs = 600\n",
    "# Batch Size\n",
    "batch_size = 256\n",
    "# RNN Size\n",
    "rnn_size = 256\n",
    "# Embedding Dimension Size\n",
    "embed_dim = 256\n",
    "# Sequence Length\n",
    "seq_length = 15\n",
    "# Learning Rate\n",
    "learning_rate = 0.001\n",
    "# Show stats for every n number of batches\n",
    "show_every_n_batches = 10\n",
    "\n",
    "save_dir = './save'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib import seq2seq\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    vocab_size = len(int_to_vocab)\n",
    "    input_text, targets, lr = get_inputs()\n",
    "    input_data_shape = tf.shape(input_text)\n",
    "    cell, initial_state = get_init_cell(input_data_shape[0], rnn_size)\n",
    "    logits, final_state = build_nn(cell, rnn_size, input_text, vocab_size, embed_dim)\n",
    "\n",
    "    # Probabilities for generating words\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    # Loss function\n",
    "    cost = seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_data_shape[0], input_data_shape[1]]))\n",
    "\n",
    "    # Optimizer\n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "\n",
    "    # Gradient Clipping\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    0/21   train_loss = 8.784\n",
      "Epoch   0 Batch   10/21   train_loss = 7.579\n",
      "Epoch   0 Batch   20/21   train_loss = 6.271\n",
      "Epoch   1 Batch    9/21   train_loss = 6.167\n",
      "Epoch   1 Batch   19/21   train_loss = 6.110\n",
      "Epoch   2 Batch    8/21   train_loss = 5.938\n",
      "Epoch   2 Batch   18/21   train_loss = 5.923\n",
      "Epoch   3 Batch    7/21   train_loss = 5.783\n",
      "Epoch   3 Batch   17/21   train_loss = 5.723\n",
      "Epoch   4 Batch    6/21   train_loss = 5.635\n",
      "Epoch   4 Batch   16/21   train_loss = 5.590\n",
      "Epoch   5 Batch    5/21   train_loss = 5.511\n",
      "Epoch   5 Batch   15/21   train_loss = 5.514\n",
      "Epoch   6 Batch    4/21   train_loss = 5.455\n",
      "Epoch   6 Batch   14/21   train_loss = 5.349\n",
      "Epoch   7 Batch    3/21   train_loss = 5.293\n",
      "Epoch   7 Batch   13/21   train_loss = 5.318\n",
      "Epoch   8 Batch    2/21   train_loss = 5.294\n",
      "Epoch   8 Batch   12/21   train_loss = 5.287\n",
      "Epoch   9 Batch    1/21   train_loss = 5.029\n",
      "Epoch   9 Batch   11/21   train_loss = 5.090\n",
      "Epoch  10 Batch    0/21   train_loss = 5.021\n",
      "Epoch  10 Batch   10/21   train_loss = 4.965\n",
      "Epoch  10 Batch   20/21   train_loss = 4.891\n",
      "Epoch  11 Batch    9/21   train_loss = 4.812\n",
      "Epoch  11 Batch   19/21   train_loss = 4.910\n",
      "Epoch  12 Batch    8/21   train_loss = 4.765\n",
      "Epoch  12 Batch   18/21   train_loss = 4.775\n",
      "Epoch  13 Batch    7/21   train_loss = 4.679\n",
      "Epoch  13 Batch   17/21   train_loss = 4.678\n",
      "Epoch  14 Batch    6/21   train_loss = 4.598\n",
      "Epoch  14 Batch   16/21   train_loss = 4.608\n",
      "Epoch  15 Batch    5/21   train_loss = 4.576\n",
      "Epoch  15 Batch   15/21   train_loss = 4.574\n",
      "Epoch  16 Batch    4/21   train_loss = 4.570\n",
      "Epoch  16 Batch   14/21   train_loss = 4.479\n",
      "Epoch  17 Batch    3/21   train_loss = 4.483\n",
      "Epoch  17 Batch   13/21   train_loss = 4.504\n",
      "Epoch  18 Batch    2/21   train_loss = 4.498\n",
      "Epoch  18 Batch   12/21   train_loss = 4.510\n",
      "Epoch  19 Batch    1/21   train_loss = 4.305\n",
      "Epoch  19 Batch   11/21   train_loss = 4.405\n",
      "Epoch  20 Batch    0/21   train_loss = 4.348\n",
      "Epoch  20 Batch   10/21   train_loss = 4.308\n",
      "Epoch  20 Batch   20/21   train_loss = 4.258\n",
      "Epoch  21 Batch    9/21   train_loss = 4.213\n",
      "Epoch  21 Batch   19/21   train_loss = 4.317\n",
      "Epoch  22 Batch    8/21   train_loss = 4.220\n",
      "Epoch  22 Batch   18/21   train_loss = 4.215\n",
      "Epoch  23 Batch    7/21   train_loss = 4.158\n",
      "Epoch  23 Batch   17/21   train_loss = 4.162\n",
      "Epoch  24 Batch    6/21   train_loss = 4.095\n",
      "Epoch  24 Batch   16/21   train_loss = 4.132\n",
      "Epoch  25 Batch    5/21   train_loss = 4.109\n",
      "Epoch  25 Batch   15/21   train_loss = 4.107\n",
      "Epoch  26 Batch    4/21   train_loss = 4.091\n",
      "Epoch  26 Batch   14/21   train_loss = 4.026\n",
      "Epoch  27 Batch    3/21   train_loss = 4.051\n",
      "Epoch  27 Batch   13/21   train_loss = 4.046\n",
      "Epoch  28 Batch    2/21   train_loss = 4.055\n",
      "Epoch  28 Batch   12/21   train_loss = 4.060\n",
      "Epoch  29 Batch    1/21   train_loss = 3.904\n",
      "Epoch  29 Batch   11/21   train_loss = 3.995\n",
      "Epoch  30 Batch    0/21   train_loss = 3.948\n",
      "Epoch  30 Batch   10/21   train_loss = 3.902\n",
      "Epoch  30 Batch   20/21   train_loss = 3.871\n",
      "Epoch  31 Batch    9/21   train_loss = 3.829\n",
      "Epoch  31 Batch   19/21   train_loss = 3.917\n",
      "Epoch  32 Batch    8/21   train_loss = 3.848\n",
      "Epoch  32 Batch   18/21   train_loss = 3.815\n",
      "Epoch  33 Batch    7/21   train_loss = 3.782\n",
      "Epoch  33 Batch   17/21   train_loss = 3.791\n",
      "Epoch  34 Batch    6/21   train_loss = 3.738\n",
      "Epoch  34 Batch   16/21   train_loss = 3.782\n",
      "Epoch  35 Batch    5/21   train_loss = 3.763\n",
      "Epoch  35 Batch   15/21   train_loss = 3.747\n",
      "Epoch  36 Batch    4/21   train_loss = 3.714\n",
      "Epoch  36 Batch   14/21   train_loss = 3.675\n",
      "Epoch  37 Batch    3/21   train_loss = 3.710\n",
      "Epoch  37 Batch   13/21   train_loss = 3.676\n",
      "Epoch  38 Batch    2/21   train_loss = 3.706\n",
      "Epoch  38 Batch   12/21   train_loss = 3.696\n",
      "Epoch  39 Batch    1/21   train_loss = 3.585\n",
      "Epoch  39 Batch   11/21   train_loss = 3.663\n",
      "Epoch  40 Batch    0/21   train_loss = 3.620\n",
      "Epoch  40 Batch   10/21   train_loss = 3.579\n",
      "Epoch  40 Batch   20/21   train_loss = 3.548\n",
      "Epoch  41 Batch    9/21   train_loss = 3.516\n",
      "Epoch  41 Batch   19/21   train_loss = 3.586\n",
      "Epoch  42 Batch    8/21   train_loss = 3.550\n",
      "Epoch  42 Batch   18/21   train_loss = 3.489\n",
      "Epoch  43 Batch    7/21   train_loss = 3.477\n",
      "Epoch  43 Batch   17/21   train_loss = 3.489\n",
      "Epoch  44 Batch    6/21   train_loss = 3.444\n",
      "Epoch  44 Batch   16/21   train_loss = 3.494\n",
      "Epoch  45 Batch    5/21   train_loss = 3.481\n",
      "Epoch  45 Batch   15/21   train_loss = 3.441\n",
      "Epoch  46 Batch    4/21   train_loss = 3.400\n",
      "Epoch  46 Batch   14/21   train_loss = 3.393\n",
      "Epoch  47 Batch    3/21   train_loss = 3.432\n",
      "Epoch  47 Batch   13/21   train_loss = 3.374\n",
      "Epoch  48 Batch    2/21   train_loss = 3.414\n",
      "Epoch  48 Batch   12/21   train_loss = 3.395\n",
      "Epoch  49 Batch    1/21   train_loss = 3.323\n",
      "Epoch  49 Batch   11/21   train_loss = 3.379\n",
      "Epoch  50 Batch    0/21   train_loss = 3.344\n",
      "Epoch  50 Batch   10/21   train_loss = 3.306\n",
      "Epoch  50 Batch   20/21   train_loss = 3.277\n",
      "Epoch  51 Batch    9/21   train_loss = 3.251\n",
      "Epoch  51 Batch   19/21   train_loss = 3.307\n",
      "Epoch  52 Batch    8/21   train_loss = 3.293\n",
      "Epoch  52 Batch   18/21   train_loss = 3.216\n",
      "Epoch  53 Batch    7/21   train_loss = 3.210\n",
      "Epoch  53 Batch   17/21   train_loss = 3.224\n",
      "Epoch  54 Batch    6/21   train_loss = 3.185\n",
      "Epoch  54 Batch   16/21   train_loss = 3.223\n",
      "Epoch  55 Batch    5/21   train_loss = 3.226\n",
      "Epoch  55 Batch   15/21   train_loss = 3.170\n",
      "Epoch  56 Batch    4/21   train_loss = 3.121\n",
      "Epoch  56 Batch   14/21   train_loss = 3.133\n",
      "Epoch  57 Batch    3/21   train_loss = 3.173\n",
      "Epoch  57 Batch   13/21   train_loss = 3.097\n",
      "Epoch  58 Batch    2/21   train_loss = 3.142\n",
      "Epoch  58 Batch   12/21   train_loss = 3.111\n",
      "Epoch  59 Batch    1/21   train_loss = 3.082\n",
      "Epoch  59 Batch   11/21   train_loss = 3.118\n",
      "Epoch  60 Batch    0/21   train_loss = 3.090\n",
      "Epoch  60 Batch   10/21   train_loss = 3.058\n",
      "Epoch  60 Batch   20/21   train_loss = 3.024\n",
      "Epoch  61 Batch    9/21   train_loss = 3.012\n",
      "Epoch  61 Batch   19/21   train_loss = 3.047\n",
      "Epoch  62 Batch    8/21   train_loss = 3.056\n",
      "Epoch  62 Batch   18/21   train_loss = 2.966\n",
      "Epoch  63 Batch    7/21   train_loss = 2.971\n",
      "Epoch  63 Batch   17/21   train_loss = 2.993\n",
      "Epoch  64 Batch    6/21   train_loss = 2.957\n",
      "Epoch  64 Batch   16/21   train_loss = 2.989\n",
      "Epoch  65 Batch    5/21   train_loss = 3.004\n",
      "Epoch  65 Batch   15/21   train_loss = 2.941\n",
      "Epoch  66 Batch    4/21   train_loss = 2.886\n",
      "Epoch  66 Batch   14/21   train_loss = 2.916\n",
      "Epoch  67 Batch    3/21   train_loss = 2.958\n",
      "Epoch  67 Batch   13/21   train_loss = 2.873\n",
      "Epoch  68 Batch    2/21   train_loss = 2.914\n",
      "Epoch  68 Batch   12/21   train_loss = 2.873\n",
      "Epoch  69 Batch    1/21   train_loss = 2.878\n",
      "Epoch  69 Batch   11/21   train_loss = 2.890\n",
      "Epoch  70 Batch    0/21   train_loss = 2.873\n",
      "Epoch  70 Batch   10/21   train_loss = 2.846\n",
      "Epoch  70 Batch   20/21   train_loss = 2.811\n",
      "Epoch  71 Batch    9/21   train_loss = 2.807\n",
      "Epoch  71 Batch   19/21   train_loss = 2.825\n",
      "Epoch  72 Batch    8/21   train_loss = 2.850\n",
      "Epoch  72 Batch   18/21   train_loss = 2.752\n",
      "Epoch  73 Batch    7/21   train_loss = 2.757\n",
      "Epoch  73 Batch   17/21   train_loss = 2.787\n",
      "Epoch  74 Batch    6/21   train_loss = 2.750\n",
      "Epoch  74 Batch   16/21   train_loss = 2.782\n",
      "Epoch  75 Batch    5/21   train_loss = 2.800\n",
      "Epoch  75 Batch   15/21   train_loss = 2.736\n",
      "Epoch  76 Batch    4/21   train_loss = 2.678\n",
      "Epoch  76 Batch   14/21   train_loss = 2.714\n",
      "Epoch  77 Batch    3/21   train_loss = 2.760\n",
      "Epoch  77 Batch   13/21   train_loss = 2.669\n",
      "Epoch  78 Batch    2/21   train_loss = 2.702\n",
      "Epoch  78 Batch   12/21   train_loss = 2.655\n",
      "Epoch  79 Batch    1/21   train_loss = 2.682\n",
      "Epoch  79 Batch   11/21   train_loss = 2.683\n",
      "Epoch  80 Batch    0/21   train_loss = 2.669\n",
      "Epoch  80 Batch   10/21   train_loss = 2.657\n",
      "Epoch  80 Batch   20/21   train_loss = 2.625\n",
      "Epoch  81 Batch    9/21   train_loss = 2.619\n",
      "Epoch  81 Batch   19/21   train_loss = 2.639\n",
      "Epoch  82 Batch    8/21   train_loss = 2.668\n",
      "Epoch  82 Batch   18/21   train_loss = 2.563\n",
      "Epoch  83 Batch    7/21   train_loss = 2.568\n",
      "Epoch  83 Batch   17/21   train_loss = 2.601\n",
      "Epoch  84 Batch    6/21   train_loss = 2.565\n",
      "Epoch  84 Batch   16/21   train_loss = 2.596\n",
      "Epoch  85 Batch    5/21   train_loss = 2.618\n",
      "Epoch  85 Batch   15/21   train_loss = 2.551\n",
      "Epoch  86 Batch    4/21   train_loss = 2.488\n",
      "Epoch  86 Batch   14/21   train_loss = 2.534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  87 Batch    3/21   train_loss = 2.582\n",
      "Epoch  87 Batch   13/21   train_loss = 2.491\n",
      "Epoch  88 Batch    2/21   train_loss = 2.510\n",
      "Epoch  88 Batch   12/21   train_loss = 2.471\n",
      "Epoch  89 Batch    1/21   train_loss = 2.516\n",
      "Epoch  89 Batch   11/21   train_loss = 2.501\n",
      "Epoch  90 Batch    0/21   train_loss = 2.499\n",
      "Epoch  90 Batch   10/21   train_loss = 2.482\n",
      "Epoch  90 Batch   20/21   train_loss = 2.457\n",
      "Epoch  91 Batch    9/21   train_loss = 2.452\n",
      "Epoch  91 Batch   19/21   train_loss = 2.459\n",
      "Epoch  92 Batch    8/21   train_loss = 2.505\n",
      "Epoch  92 Batch   18/21   train_loss = 2.395\n",
      "Epoch  93 Batch    7/21   train_loss = 2.399\n",
      "Epoch  93 Batch   17/21   train_loss = 2.440\n",
      "Epoch  94 Batch    6/21   train_loss = 2.397\n",
      "Epoch  94 Batch   16/21   train_loss = 2.419\n",
      "Epoch  95 Batch    5/21   train_loss = 2.456\n",
      "Epoch  95 Batch   15/21   train_loss = 2.371\n",
      "Epoch  96 Batch    4/21   train_loss = 2.320\n",
      "Epoch  96 Batch   14/21   train_loss = 2.354\n",
      "Epoch  97 Batch    3/21   train_loss = 2.404\n",
      "Epoch  97 Batch   13/21   train_loss = 2.309\n",
      "Epoch  98 Batch    2/21   train_loss = 2.314\n",
      "Epoch  98 Batch   12/21   train_loss = 2.275\n",
      "Epoch  99 Batch    1/21   train_loss = 2.344\n",
      "Epoch  99 Batch   11/21   train_loss = 2.303\n",
      "Epoch 100 Batch    0/21   train_loss = 2.308\n",
      "Epoch 100 Batch   10/21   train_loss = 2.290\n",
      "Epoch 100 Batch   20/21   train_loss = 2.276\n",
      "Epoch 101 Batch    9/21   train_loss = 2.265\n",
      "Epoch 101 Batch   19/21   train_loss = 2.268\n",
      "Epoch 102 Batch    8/21   train_loss = 2.316\n",
      "Epoch 102 Batch   18/21   train_loss = 2.207\n",
      "Epoch 103 Batch    7/21   train_loss = 2.215\n",
      "Epoch 103 Batch   17/21   train_loss = 2.254\n",
      "Epoch 104 Batch    6/21   train_loss = 2.227\n",
      "Epoch 104 Batch   16/21   train_loss = 2.238\n",
      "Epoch 105 Batch    5/21   train_loss = 2.285\n",
      "Epoch 105 Batch   15/21   train_loss = 2.203\n",
      "Epoch 106 Batch    4/21   train_loss = 2.147\n",
      "Epoch 106 Batch   14/21   train_loss = 2.179\n",
      "Epoch 107 Batch    3/21   train_loss = 2.237\n",
      "Epoch 107 Batch   13/21   train_loss = 2.143\n",
      "Epoch 108 Batch    2/21   train_loss = 2.141\n",
      "Epoch 108 Batch   12/21   train_loss = 2.116\n",
      "Epoch 109 Batch    1/21   train_loss = 2.196\n",
      "Epoch 109 Batch   11/21   train_loss = 2.155\n",
      "Epoch 110 Batch    0/21   train_loss = 2.177\n",
      "Epoch 110 Batch   10/21   train_loss = 2.152\n",
      "Epoch 110 Batch   20/21   train_loss = 2.140\n",
      "Epoch 111 Batch    9/21   train_loss = 2.125\n",
      "Epoch 111 Batch   19/21   train_loss = 2.109\n",
      "Epoch 112 Batch    8/21   train_loss = 2.172\n",
      "Epoch 112 Batch   18/21   train_loss = 2.058\n",
      "Epoch 113 Batch    7/21   train_loss = 2.074\n",
      "Epoch 113 Batch   17/21   train_loss = 2.109\n",
      "Epoch 114 Batch    6/21   train_loss = 2.077\n",
      "Epoch 114 Batch   16/21   train_loss = 2.086\n",
      "Epoch 115 Batch    5/21   train_loss = 2.133\n",
      "Epoch 115 Batch   15/21   train_loss = 2.048\n",
      "Epoch 116 Batch    4/21   train_loss = 1.991\n",
      "Epoch 116 Batch   14/21   train_loss = 2.020\n",
      "Epoch 117 Batch    3/21   train_loss = 2.074\n",
      "Epoch 117 Batch   13/21   train_loss = 1.988\n",
      "Epoch 118 Batch    2/21   train_loss = 1.977\n",
      "Epoch 118 Batch   12/21   train_loss = 1.947\n",
      "Epoch 119 Batch    1/21   train_loss = 2.024\n",
      "Epoch 119 Batch   11/21   train_loss = 1.978\n",
      "Epoch 120 Batch    0/21   train_loss = 1.980\n",
      "Epoch 120 Batch   10/21   train_loss = 1.970\n",
      "Epoch 120 Batch   20/21   train_loss = 1.959\n",
      "Epoch 121 Batch    9/21   train_loss = 1.952\n",
      "Epoch 121 Batch   19/21   train_loss = 1.940\n",
      "Epoch 122 Batch    8/21   train_loss = 2.010\n",
      "Epoch 122 Batch   18/21   train_loss = 1.898\n",
      "Epoch 123 Batch    7/21   train_loss = 1.908\n",
      "Epoch 123 Batch   17/21   train_loss = 1.941\n",
      "Epoch 124 Batch    6/21   train_loss = 1.907\n",
      "Epoch 124 Batch   16/21   train_loss = 1.916\n",
      "Epoch 125 Batch    5/21   train_loss = 1.974\n",
      "Epoch 125 Batch   15/21   train_loss = 1.886\n",
      "Epoch 126 Batch    4/21   train_loss = 1.838\n",
      "Epoch 126 Batch   14/21   train_loss = 1.865\n",
      "Epoch 127 Batch    3/21   train_loss = 1.924\n",
      "Epoch 127 Batch   13/21   train_loss = 1.837\n",
      "Epoch 128 Batch    2/21   train_loss = 1.825\n",
      "Epoch 128 Batch   12/21   train_loss = 1.785\n",
      "Epoch 129 Batch    1/21   train_loss = 1.884\n",
      "Epoch 129 Batch   11/21   train_loss = 1.834\n",
      "Epoch 130 Batch    0/21   train_loss = 1.830\n",
      "Epoch 130 Batch   10/21   train_loss = 1.829\n",
      "Epoch 130 Batch   20/21   train_loss = 1.821\n",
      "Epoch 131 Batch    9/21   train_loss = 1.802\n",
      "Epoch 131 Batch   19/21   train_loss = 1.790\n",
      "Epoch 132 Batch    8/21   train_loss = 1.868\n",
      "Epoch 132 Batch   18/21   train_loss = 1.759\n",
      "Epoch 133 Batch    7/21   train_loss = 1.767\n",
      "Epoch 133 Batch   17/21   train_loss = 1.799\n",
      "Epoch 134 Batch    6/21   train_loss = 1.769\n",
      "Epoch 134 Batch   16/21   train_loss = 1.764\n",
      "Epoch 135 Batch    5/21   train_loss = 1.833\n",
      "Epoch 135 Batch   15/21   train_loss = 1.747\n",
      "Epoch 136 Batch    4/21   train_loss = 1.704\n",
      "Epoch 136 Batch   14/21   train_loss = 1.735\n",
      "Epoch 137 Batch    3/21   train_loss = 1.790\n",
      "Epoch 137 Batch   13/21   train_loss = 1.706\n",
      "Epoch 138 Batch    2/21   train_loss = 1.682\n",
      "Epoch 138 Batch   12/21   train_loss = 1.647\n",
      "Epoch 139 Batch    1/21   train_loss = 1.738\n",
      "Epoch 139 Batch   11/21   train_loss = 1.696\n",
      "Epoch 140 Batch    0/21   train_loss = 1.688\n",
      "Epoch 140 Batch   10/21   train_loss = 1.694\n",
      "Epoch 140 Batch   20/21   train_loss = 1.679\n",
      "Epoch 141 Batch    9/21   train_loss = 1.665\n",
      "Epoch 141 Batch   19/21   train_loss = 1.642\n",
      "Epoch 142 Batch    8/21   train_loss = 1.710\n",
      "Epoch 142 Batch   18/21   train_loss = 1.615\n",
      "Epoch 143 Batch    7/21   train_loss = 1.625\n",
      "Epoch 143 Batch   17/21   train_loss = 1.660\n",
      "Epoch 144 Batch    6/21   train_loss = 1.643\n",
      "Epoch 144 Batch   16/21   train_loss = 1.641\n",
      "Epoch 145 Batch    5/21   train_loss = 1.721\n",
      "Epoch 145 Batch   15/21   train_loss = 1.629\n",
      "Epoch 146 Batch    4/21   train_loss = 1.595\n",
      "Epoch 146 Batch   14/21   train_loss = 1.611\n",
      "Epoch 147 Batch    3/21   train_loss = 1.659\n",
      "Epoch 147 Batch   13/21   train_loss = 1.582\n",
      "Epoch 148 Batch    2/21   train_loss = 1.550\n",
      "Epoch 148 Batch   12/21   train_loss = 1.518\n",
      "Epoch 149 Batch    1/21   train_loss = 1.614\n",
      "Epoch 149 Batch   11/21   train_loss = 1.574\n",
      "Epoch 150 Batch    0/21   train_loss = 1.564\n",
      "Epoch 150 Batch   10/21   train_loss = 1.591\n",
      "Epoch 150 Batch   20/21   train_loss = 1.597\n",
      "Epoch 151 Batch    9/21   train_loss = 1.587\n",
      "Epoch 151 Batch   19/21   train_loss = 1.570\n",
      "Epoch 152 Batch    8/21   train_loss = 1.652\n",
      "Epoch 152 Batch   18/21   train_loss = 1.549\n",
      "Epoch 153 Batch    7/21   train_loss = 1.547\n",
      "Epoch 153 Batch   17/21   train_loss = 1.564\n",
      "Epoch 154 Batch    6/21   train_loss = 1.530\n",
      "Epoch 154 Batch   16/21   train_loss = 1.528\n",
      "Epoch 155 Batch    5/21   train_loss = 1.595\n",
      "Epoch 155 Batch   15/21   train_loss = 1.522\n",
      "Epoch 156 Batch    4/21   train_loss = 1.485\n",
      "Epoch 156 Batch   14/21   train_loss = 1.495\n",
      "Epoch 157 Batch    3/21   train_loss = 1.551\n",
      "Epoch 157 Batch   13/21   train_loss = 1.469\n",
      "Epoch 158 Batch    2/21   train_loss = 1.431\n",
      "Epoch 158 Batch   12/21   train_loss = 1.396\n",
      "Epoch 159 Batch    1/21   train_loss = 1.487\n",
      "Epoch 159 Batch   11/21   train_loss = 1.438\n",
      "Epoch 160 Batch    0/21   train_loss = 1.417\n",
      "Epoch 160 Batch   10/21   train_loss = 1.429\n",
      "Epoch 160 Batch   20/21   train_loss = 1.430\n",
      "Epoch 161 Batch    9/21   train_loss = 1.412\n",
      "Epoch 161 Batch   19/21   train_loss = 1.381\n",
      "Epoch 162 Batch    8/21   train_loss = 1.459\n",
      "Epoch 162 Batch   18/21   train_loss = 1.373\n",
      "Epoch 163 Batch    7/21   train_loss = 1.381\n",
      "Epoch 163 Batch   17/21   train_loss = 1.401\n",
      "Epoch 164 Batch    6/21   train_loss = 1.388\n",
      "Epoch 164 Batch   16/21   train_loss = 1.371\n",
      "Epoch 165 Batch    5/21   train_loss = 1.438\n",
      "Epoch 165 Batch   15/21   train_loss = 1.362\n",
      "Epoch 166 Batch    4/21   train_loss = 1.326\n",
      "Epoch 166 Batch   14/21   train_loss = 1.343\n",
      "Epoch 167 Batch    3/21   train_loss = 1.384\n",
      "Epoch 167 Batch   13/21   train_loss = 1.327\n",
      "Epoch 168 Batch    2/21   train_loss = 1.293\n",
      "Epoch 168 Batch   12/21   train_loss = 1.269\n",
      "Epoch 169 Batch    1/21   train_loss = 1.362\n",
      "Epoch 169 Batch   11/21   train_loss = 1.327\n",
      "Epoch 170 Batch    0/21   train_loss = 1.303\n",
      "Epoch 170 Batch   10/21   train_loss = 1.318\n",
      "Epoch 170 Batch   20/21   train_loss = 1.317\n",
      "Epoch 171 Batch    9/21   train_loss = 1.302\n",
      "Epoch 171 Batch   19/21   train_loss = 1.271\n",
      "Epoch 172 Batch    8/21   train_loss = 1.345\n",
      "Epoch 172 Batch   18/21   train_loss = 1.273\n",
      "Epoch 173 Batch    7/21   train_loss = 1.279\n",
      "Epoch 173 Batch   17/21   train_loss = 1.305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 174 Batch    6/21   train_loss = 1.290\n",
      "Epoch 174 Batch   16/21   train_loss = 1.259\n",
      "Epoch 175 Batch    5/21   train_loss = 1.330\n",
      "Epoch 175 Batch   15/21   train_loss = 1.252\n",
      "Epoch 176 Batch    4/21   train_loss = 1.229\n",
      "Epoch 176 Batch   14/21   train_loss = 1.242\n",
      "Epoch 177 Batch    3/21   train_loss = 1.284\n",
      "Epoch 177 Batch   13/21   train_loss = 1.225\n",
      "Epoch 178 Batch    2/21   train_loss = 1.200\n",
      "Epoch 178 Batch   12/21   train_loss = 1.165\n",
      "Epoch 179 Batch    1/21   train_loss = 1.253\n",
      "Epoch 179 Batch   11/21   train_loss = 1.221\n",
      "Epoch 180 Batch    0/21   train_loss = 1.191\n",
      "Epoch 180 Batch   10/21   train_loss = 1.215\n",
      "Epoch 180 Batch   20/21   train_loss = 1.210\n",
      "Epoch 181 Batch    9/21   train_loss = 1.199\n",
      "Epoch 181 Batch   19/21   train_loss = 1.166\n",
      "Epoch 182 Batch    8/21   train_loss = 1.233\n",
      "Epoch 182 Batch   18/21   train_loss = 1.168\n",
      "Epoch 183 Batch    7/21   train_loss = 1.171\n",
      "Epoch 183 Batch   17/21   train_loss = 1.198\n",
      "Epoch 184 Batch    6/21   train_loss = 1.182\n",
      "Epoch 184 Batch   16/21   train_loss = 1.148\n",
      "Epoch 185 Batch    5/21   train_loss = 1.213\n",
      "Epoch 185 Batch   15/21   train_loss = 1.143\n",
      "Epoch 186 Batch    4/21   train_loss = 1.122\n",
      "Epoch 186 Batch   14/21   train_loss = 1.130\n",
      "Epoch 187 Batch    3/21   train_loss = 1.174\n",
      "Epoch 187 Batch   13/21   train_loss = 1.122\n",
      "Epoch 188 Batch    2/21   train_loss = 1.093\n",
      "Epoch 188 Batch   12/21   train_loss = 1.062\n",
      "Epoch 189 Batch    1/21   train_loss = 1.152\n",
      "Epoch 189 Batch   11/21   train_loss = 1.122\n",
      "Epoch 190 Batch    0/21   train_loss = 1.092\n",
      "Epoch 190 Batch   10/21   train_loss = 1.119\n",
      "Epoch 190 Batch   20/21   train_loss = 1.108\n",
      "Epoch 191 Batch    9/21   train_loss = 1.098\n",
      "Epoch 191 Batch   19/21   train_loss = 1.061\n",
      "Epoch 192 Batch    8/21   train_loss = 1.129\n",
      "Epoch 192 Batch   18/21   train_loss = 1.072\n",
      "Epoch 193 Batch    7/21   train_loss = 1.080\n",
      "Epoch 193 Batch   17/21   train_loss = 1.095\n",
      "Epoch 194 Batch    6/21   train_loss = 1.082\n",
      "Epoch 194 Batch   16/21   train_loss = 1.050\n",
      "Epoch 195 Batch    5/21   train_loss = 1.107\n",
      "Epoch 195 Batch   15/21   train_loss = 1.047\n",
      "Epoch 196 Batch    4/21   train_loss = 1.027\n",
      "Epoch 196 Batch   14/21   train_loss = 1.040\n",
      "Epoch 197 Batch    3/21   train_loss = 1.076\n",
      "Epoch 197 Batch   13/21   train_loss = 1.034\n",
      "Epoch 198 Batch    2/21   train_loss = 1.007\n",
      "Epoch 198 Batch   12/21   train_loss = 0.972\n",
      "Epoch 199 Batch    1/21   train_loss = 1.063\n",
      "Epoch 199 Batch   11/21   train_loss = 1.029\n",
      "Epoch 200 Batch    0/21   train_loss = 0.999\n",
      "Epoch 200 Batch   10/21   train_loss = 1.031\n",
      "Epoch 200 Batch   20/21   train_loss = 1.033\n",
      "Epoch 201 Batch    9/21   train_loss = 1.015\n",
      "Epoch 201 Batch   19/21   train_loss = 0.970\n",
      "Epoch 202 Batch    8/21   train_loss = 1.044\n",
      "Epoch 202 Batch   18/21   train_loss = 0.993\n",
      "Epoch 203 Batch    7/21   train_loss = 0.996\n",
      "Epoch 203 Batch   17/21   train_loss = 1.017\n",
      "Epoch 204 Batch    6/21   train_loss = 1.003\n",
      "Epoch 204 Batch   16/21   train_loss = 0.968\n",
      "Epoch 205 Batch    5/21   train_loss = 1.035\n",
      "Epoch 205 Batch   15/21   train_loss = 0.985\n",
      "Epoch 206 Batch    4/21   train_loss = 0.961\n",
      "Epoch 206 Batch   14/21   train_loss = 0.967\n",
      "Epoch 207 Batch    3/21   train_loss = 1.005\n",
      "Epoch 207 Batch   13/21   train_loss = 0.982\n",
      "Epoch 208 Batch    2/21   train_loss = 0.944\n",
      "Epoch 208 Batch   12/21   train_loss = 0.900\n",
      "Epoch 209 Batch    1/21   train_loss = 0.997\n",
      "Epoch 209 Batch   11/21   train_loss = 0.966\n",
      "Epoch 210 Batch    0/21   train_loss = 0.924\n",
      "Epoch 210 Batch   10/21   train_loss = 0.957\n",
      "Epoch 210 Batch   20/21   train_loss = 0.958\n",
      "Epoch 211 Batch    9/21   train_loss = 0.951\n",
      "Epoch 211 Batch   19/21   train_loss = 0.901\n",
      "Epoch 212 Batch    8/21   train_loss = 0.973\n",
      "Epoch 212 Batch   18/21   train_loss = 0.929\n",
      "Epoch 213 Batch    7/21   train_loss = 0.917\n",
      "Epoch 213 Batch   17/21   train_loss = 0.939\n",
      "Epoch 214 Batch    6/21   train_loss = 0.931\n",
      "Epoch 214 Batch   16/21   train_loss = 0.890\n",
      "Epoch 215 Batch    5/21   train_loss = 0.945\n",
      "Epoch 215 Batch   15/21   train_loss = 0.899\n",
      "Epoch 216 Batch    4/21   train_loss = 0.881\n",
      "Epoch 216 Batch   14/21   train_loss = 0.889\n",
      "Epoch 217 Batch    3/21   train_loss = 0.923\n",
      "Epoch 217 Batch   13/21   train_loss = 0.897\n",
      "Epoch 218 Batch    2/21   train_loss = 0.864\n",
      "Epoch 218 Batch   12/21   train_loss = 0.830\n",
      "Epoch 219 Batch    1/21   train_loss = 0.907\n",
      "Epoch 219 Batch   11/21   train_loss = 0.880\n",
      "Epoch 220 Batch    0/21   train_loss = 0.842\n",
      "Epoch 220 Batch   10/21   train_loss = 0.883\n",
      "Epoch 220 Batch   20/21   train_loss = 0.884\n",
      "Epoch 221 Batch    9/21   train_loss = 0.875\n",
      "Epoch 221 Batch   19/21   train_loss = 0.839\n",
      "Epoch 222 Batch    8/21   train_loss = 0.897\n",
      "Epoch 222 Batch   18/21   train_loss = 0.865\n",
      "Epoch 223 Batch    7/21   train_loss = 0.851\n",
      "Epoch 223 Batch   17/21   train_loss = 0.877\n",
      "Epoch 224 Batch    6/21   train_loss = 0.858\n",
      "Epoch 224 Batch   16/21   train_loss = 0.826\n",
      "Epoch 225 Batch    5/21   train_loss = 0.872\n",
      "Epoch 225 Batch   15/21   train_loss = 0.832\n",
      "Epoch 226 Batch    4/21   train_loss = 0.819\n",
      "Epoch 226 Batch   14/21   train_loss = 0.817\n",
      "Epoch 227 Batch    3/21   train_loss = 0.846\n",
      "Epoch 227 Batch   13/21   train_loss = 0.834\n",
      "Epoch 228 Batch    2/21   train_loss = 0.796\n",
      "Epoch 228 Batch   12/21   train_loss = 0.764\n",
      "Epoch 229 Batch    1/21   train_loss = 0.840\n",
      "Epoch 229 Batch   11/21   train_loss = 0.812\n",
      "Epoch 230 Batch    0/21   train_loss = 0.784\n",
      "Epoch 230 Batch   10/21   train_loss = 0.818\n",
      "Epoch 230 Batch   20/21   train_loss = 0.832\n",
      "Epoch 231 Batch    9/21   train_loss = 0.829\n",
      "Epoch 231 Batch   19/21   train_loss = 0.793\n",
      "Epoch 232 Batch    8/21   train_loss = 0.844\n",
      "Epoch 232 Batch   18/21   train_loss = 0.812\n",
      "Epoch 233 Batch    7/21   train_loss = 0.804\n",
      "Epoch 233 Batch   17/21   train_loss = 0.823\n",
      "Epoch 234 Batch    6/21   train_loss = 0.803\n",
      "Epoch 234 Batch   16/21   train_loss = 0.768\n",
      "Epoch 235 Batch    5/21   train_loss = 0.811\n",
      "Epoch 235 Batch   15/21   train_loss = 0.778\n",
      "Epoch 236 Batch    4/21   train_loss = 0.759\n",
      "Epoch 236 Batch   14/21   train_loss = 0.767\n",
      "Epoch 237 Batch    3/21   train_loss = 0.789\n",
      "Epoch 237 Batch   13/21   train_loss = 0.788\n",
      "Epoch 238 Batch    2/21   train_loss = 0.753\n",
      "Epoch 238 Batch   12/21   train_loss = 0.726\n",
      "Epoch 239 Batch    1/21   train_loss = 0.792\n",
      "Epoch 239 Batch   11/21   train_loss = 0.763\n",
      "Epoch 240 Batch    0/21   train_loss = 0.742\n",
      "Epoch 240 Batch   10/21   train_loss = 0.767\n",
      "Epoch 240 Batch   20/21   train_loss = 0.767\n",
      "Epoch 241 Batch    9/21   train_loss = 0.763\n",
      "Epoch 241 Batch   19/21   train_loss = 0.729\n",
      "Epoch 242 Batch    8/21   train_loss = 0.777\n",
      "Epoch 242 Batch   18/21   train_loss = 0.755\n",
      "Epoch 243 Batch    7/21   train_loss = 0.746\n",
      "Epoch 243 Batch   17/21   train_loss = 0.771\n",
      "Epoch 244 Batch    6/21   train_loss = 0.748\n",
      "Epoch 244 Batch   16/21   train_loss = 0.707\n",
      "Epoch 245 Batch    5/21   train_loss = 0.750\n",
      "Epoch 245 Batch   15/21   train_loss = 0.720\n",
      "Epoch 246 Batch    4/21   train_loss = 0.707\n",
      "Epoch 246 Batch   14/21   train_loss = 0.705\n",
      "Epoch 247 Batch    3/21   train_loss = 0.728\n",
      "Epoch 247 Batch   13/21   train_loss = 0.732\n",
      "Epoch 248 Batch    2/21   train_loss = 0.703\n",
      "Epoch 248 Batch   12/21   train_loss = 0.670\n",
      "Epoch 249 Batch    1/21   train_loss = 0.731\n",
      "Epoch 249 Batch   11/21   train_loss = 0.709\n",
      "Epoch 250 Batch    0/21   train_loss = 0.682\n",
      "Epoch 250 Batch   10/21   train_loss = 0.713\n",
      "Epoch 250 Batch   20/21   train_loss = 0.713\n",
      "Epoch 251 Batch    9/21   train_loss = 0.699\n",
      "Epoch 251 Batch   19/21   train_loss = 0.667\n",
      "Epoch 252 Batch    8/21   train_loss = 0.716\n",
      "Epoch 252 Batch   18/21   train_loss = 0.694\n",
      "Epoch 253 Batch    7/21   train_loss = 0.681\n",
      "Epoch 253 Batch   17/21   train_loss = 0.709\n",
      "Epoch 254 Batch    6/21   train_loss = 0.701\n",
      "Epoch 254 Batch   16/21   train_loss = 0.662\n",
      "Epoch 255 Batch    5/21   train_loss = 0.712\n",
      "Epoch 255 Batch   15/21   train_loss = 0.673\n",
      "Epoch 256 Batch    4/21   train_loss = 0.662\n",
      "Epoch 256 Batch   14/21   train_loss = 0.653\n",
      "Epoch 257 Batch    3/21   train_loss = 0.667\n",
      "Epoch 257 Batch   13/21   train_loss = 0.669\n",
      "Epoch 258 Batch    2/21   train_loss = 0.644\n",
      "Epoch 258 Batch   12/21   train_loss = 0.609\n",
      "Epoch 259 Batch    1/21   train_loss = 0.671\n",
      "Epoch 259 Batch   11/21   train_loss = 0.656\n",
      "Epoch 260 Batch    0/21   train_loss = 0.626\n",
      "Epoch 260 Batch   10/21   train_loss = 0.665\n",
      "Epoch 260 Batch   20/21   train_loss = 0.666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 261 Batch    9/21   train_loss = 0.658\n",
      "Epoch 261 Batch   19/21   train_loss = 0.618\n",
      "Epoch 262 Batch    8/21   train_loss = 0.669\n",
      "Epoch 262 Batch   18/21   train_loss = 0.644\n",
      "Epoch 263 Batch    7/21   train_loss = 0.623\n",
      "Epoch 263 Batch   17/21   train_loss = 0.640\n",
      "Epoch 264 Batch    6/21   train_loss = 0.617\n",
      "Epoch 264 Batch   16/21   train_loss = 0.574\n",
      "Epoch 265 Batch    5/21   train_loss = 0.619\n",
      "Epoch 265 Batch   15/21   train_loss = 0.596\n",
      "Epoch 266 Batch    4/21   train_loss = 0.588\n",
      "Epoch 266 Batch   14/21   train_loss = 0.592\n",
      "Epoch 267 Batch    3/21   train_loss = 0.603\n",
      "Epoch 267 Batch   13/21   train_loss = 0.604\n",
      "Epoch 268 Batch    2/21   train_loss = 0.579\n",
      "Epoch 268 Batch   12/21   train_loss = 0.549\n",
      "Epoch 269 Batch    1/21   train_loss = 0.599\n",
      "Epoch 269 Batch   11/21   train_loss = 0.584\n",
      "Epoch 270 Batch    0/21   train_loss = 0.554\n",
      "Epoch 270 Batch   10/21   train_loss = 0.592\n",
      "Epoch 270 Batch   20/21   train_loss = 0.590\n",
      "Epoch 271 Batch    9/21   train_loss = 0.584\n",
      "Epoch 271 Batch   19/21   train_loss = 0.553\n",
      "Epoch 272 Batch    8/21   train_loss = 0.596\n",
      "Epoch 272 Batch   18/21   train_loss = 0.579\n",
      "Epoch 273 Batch    7/21   train_loss = 0.565\n",
      "Epoch 273 Batch   17/21   train_loss = 0.589\n",
      "Epoch 274 Batch    6/21   train_loss = 0.569\n",
      "Epoch 274 Batch   16/21   train_loss = 0.524\n",
      "Epoch 275 Batch    5/21   train_loss = 0.566\n",
      "Epoch 275 Batch   15/21   train_loss = 0.549\n",
      "Epoch 276 Batch    4/21   train_loss = 0.540\n",
      "Epoch 276 Batch   14/21   train_loss = 0.545\n",
      "Epoch 277 Batch    3/21   train_loss = 0.560\n",
      "Epoch 277 Batch   13/21   train_loss = 0.563\n",
      "Epoch 278 Batch    2/21   train_loss = 0.544\n",
      "Epoch 278 Batch   12/21   train_loss = 0.515\n",
      "Epoch 279 Batch    1/21   train_loss = 0.559\n",
      "Epoch 279 Batch   11/21   train_loss = 0.551\n",
      "Epoch 280 Batch    0/21   train_loss = 0.523\n",
      "Epoch 280 Batch   10/21   train_loss = 0.559\n",
      "Epoch 280 Batch   20/21   train_loss = 0.563\n",
      "Epoch 281 Batch    9/21   train_loss = 0.553\n",
      "Epoch 281 Batch   19/21   train_loss = 0.520\n",
      "Epoch 282 Batch    8/21   train_loss = 0.558\n",
      "Epoch 282 Batch   18/21   train_loss = 0.550\n",
      "Epoch 283 Batch    7/21   train_loss = 0.542\n",
      "Epoch 283 Batch   17/21   train_loss = 0.570\n",
      "Epoch 284 Batch    6/21   train_loss = 0.540\n",
      "Epoch 284 Batch   16/21   train_loss = 0.508\n",
      "Epoch 285 Batch    5/21   train_loss = 0.562\n",
      "Epoch 285 Batch   15/21   train_loss = 0.527\n",
      "Epoch 286 Batch    4/21   train_loss = 0.524\n",
      "Epoch 286 Batch   14/21   train_loss = 0.535\n",
      "Epoch 287 Batch    3/21   train_loss = 0.533\n",
      "Epoch 287 Batch   13/21   train_loss = 0.540\n",
      "Epoch 288 Batch    2/21   train_loss = 0.524\n",
      "Epoch 288 Batch   12/21   train_loss = 0.484\n",
      "Epoch 289 Batch    1/21   train_loss = 0.523\n",
      "Epoch 289 Batch   11/21   train_loss = 0.522\n",
      "Epoch 290 Batch    0/21   train_loss = 0.487\n",
      "Epoch 290 Batch   10/21   train_loss = 0.526\n",
      "Epoch 290 Batch   20/21   train_loss = 0.525\n",
      "Epoch 291 Batch    9/21   train_loss = 0.512\n",
      "Epoch 291 Batch   19/21   train_loss = 0.493\n",
      "Epoch 292 Batch    8/21   train_loss = 0.534\n",
      "Epoch 292 Batch   18/21   train_loss = 0.519\n",
      "Epoch 293 Batch    7/21   train_loss = 0.509\n",
      "Epoch 293 Batch   17/21   train_loss = 0.530\n",
      "Epoch 294 Batch    6/21   train_loss = 0.513\n",
      "Epoch 294 Batch   16/21   train_loss = 0.465\n",
      "Epoch 295 Batch    5/21   train_loss = 0.505\n",
      "Epoch 295 Batch   15/21   train_loss = 0.494\n",
      "Epoch 296 Batch    4/21   train_loss = 0.480\n",
      "Epoch 296 Batch   14/21   train_loss = 0.482\n",
      "Epoch 297 Batch    3/21   train_loss = 0.497\n",
      "Epoch 297 Batch   13/21   train_loss = 0.500\n",
      "Epoch 298 Batch    2/21   train_loss = 0.476\n",
      "Epoch 298 Batch   12/21   train_loss = 0.454\n",
      "Epoch 299 Batch    1/21   train_loss = 0.492\n",
      "Epoch 299 Batch   11/21   train_loss = 0.489\n",
      "Epoch 300 Batch    0/21   train_loss = 0.463\n",
      "Epoch 300 Batch   10/21   train_loss = 0.507\n",
      "Epoch 300 Batch   20/21   train_loss = 0.504\n",
      "Epoch 301 Batch    9/21   train_loss = 0.493\n",
      "Epoch 301 Batch   19/21   train_loss = 0.473\n",
      "Epoch 302 Batch    8/21   train_loss = 0.512\n",
      "Epoch 302 Batch   18/21   train_loss = 0.493\n",
      "Epoch 303 Batch    7/21   train_loss = 0.481\n",
      "Epoch 303 Batch   17/21   train_loss = 0.509\n",
      "Epoch 304 Batch    6/21   train_loss = 0.472\n",
      "Epoch 304 Batch   16/21   train_loss = 0.436\n",
      "Epoch 305 Batch    5/21   train_loss = 0.483\n",
      "Epoch 305 Batch   15/21   train_loss = 0.463\n",
      "Epoch 306 Batch    4/21   train_loss = 0.459\n",
      "Epoch 306 Batch   14/21   train_loss = 0.464\n",
      "Epoch 307 Batch    3/21   train_loss = 0.484\n",
      "Epoch 307 Batch   13/21   train_loss = 0.497\n",
      "Epoch 308 Batch    2/21   train_loss = 0.475\n",
      "Epoch 308 Batch   12/21   train_loss = 0.454\n",
      "Epoch 309 Batch    1/21   train_loss = 0.479\n",
      "Epoch 309 Batch   11/21   train_loss = 0.471\n",
      "Epoch 310 Batch    0/21   train_loss = 0.439\n",
      "Epoch 310 Batch   10/21   train_loss = 0.474\n",
      "Epoch 310 Batch   20/21   train_loss = 0.469\n",
      "Epoch 311 Batch    9/21   train_loss = 0.464\n",
      "Epoch 311 Batch   19/21   train_loss = 0.447\n",
      "Epoch 312 Batch    8/21   train_loss = 0.479\n",
      "Epoch 312 Batch   18/21   train_loss = 0.479\n",
      "Epoch 313 Batch    7/21   train_loss = 0.462\n",
      "Epoch 313 Batch   17/21   train_loss = 0.488\n",
      "Epoch 314 Batch    6/21   train_loss = 0.459\n",
      "Epoch 314 Batch   16/21   train_loss = 0.415\n",
      "Epoch 315 Batch    5/21   train_loss = 0.461\n",
      "Epoch 315 Batch   15/21   train_loss = 0.443\n",
      "Epoch 316 Batch    4/21   train_loss = 0.432\n",
      "Epoch 316 Batch   14/21   train_loss = 0.430\n",
      "Epoch 317 Batch    3/21   train_loss = 0.439\n",
      "Epoch 317 Batch   13/21   train_loss = 0.444\n",
      "Epoch 318 Batch    2/21   train_loss = 0.424\n",
      "Epoch 318 Batch   12/21   train_loss = 0.403\n",
      "Epoch 319 Batch    1/21   train_loss = 0.432\n",
      "Epoch 319 Batch   11/21   train_loss = 0.428\n",
      "Epoch 320 Batch    0/21   train_loss = 0.408\n",
      "Epoch 320 Batch   10/21   train_loss = 0.452\n",
      "Epoch 320 Batch   20/21   train_loss = 0.451\n",
      "Epoch 321 Batch    9/21   train_loss = 0.435\n",
      "Epoch 321 Batch   19/21   train_loss = 0.413\n",
      "Epoch 322 Batch    8/21   train_loss = 0.445\n",
      "Epoch 322 Batch   18/21   train_loss = 0.442\n",
      "Epoch 323 Batch    7/21   train_loss = 0.416\n",
      "Epoch 323 Batch   17/21   train_loss = 0.445\n",
      "Epoch 324 Batch    6/21   train_loss = 0.422\n",
      "Epoch 324 Batch   16/21   train_loss = 0.385\n",
      "Epoch 325 Batch    5/21   train_loss = 0.424\n",
      "Epoch 325 Batch   15/21   train_loss = 0.410\n",
      "Epoch 326 Batch    4/21   train_loss = 0.399\n",
      "Epoch 326 Batch   14/21   train_loss = 0.397\n",
      "Epoch 327 Batch    3/21   train_loss = 0.407\n",
      "Epoch 327 Batch   13/21   train_loss = 0.414\n",
      "Epoch 328 Batch    2/21   train_loss = 0.399\n",
      "Epoch 328 Batch   12/21   train_loss = 0.384\n",
      "Epoch 329 Batch    1/21   train_loss = 0.414\n",
      "Epoch 329 Batch   11/21   train_loss = 0.410\n",
      "Epoch 330 Batch    0/21   train_loss = 0.384\n",
      "Epoch 330 Batch   10/21   train_loss = 0.427\n",
      "Epoch 330 Batch   20/21   train_loss = 0.427\n",
      "Epoch 331 Batch    9/21   train_loss = 0.411\n",
      "Epoch 331 Batch   19/21   train_loss = 0.392\n",
      "Epoch 332 Batch    8/21   train_loss = 0.427\n",
      "Epoch 332 Batch   18/21   train_loss = 0.420\n",
      "Epoch 333 Batch    7/21   train_loss = 0.397\n",
      "Epoch 333 Batch   17/21   train_loss = 0.425\n",
      "Epoch 334 Batch    6/21   train_loss = 0.393\n",
      "Epoch 334 Batch   16/21   train_loss = 0.353\n",
      "Epoch 335 Batch    5/21   train_loss = 0.397\n",
      "Epoch 335 Batch   15/21   train_loss = 0.387\n",
      "Epoch 336 Batch    4/21   train_loss = 0.377\n",
      "Epoch 336 Batch   14/21   train_loss = 0.385\n",
      "Epoch 337 Batch    3/21   train_loss = 0.397\n",
      "Epoch 337 Batch   13/21   train_loss = 0.406\n",
      "Epoch 338 Batch    2/21   train_loss = 0.394\n",
      "Epoch 338 Batch   12/21   train_loss = 0.376\n",
      "Epoch 339 Batch    1/21   train_loss = 0.406\n",
      "Epoch 339 Batch   11/21   train_loss = 0.404\n",
      "Epoch 340 Batch    0/21   train_loss = 0.381\n",
      "Epoch 340 Batch   10/21   train_loss = 0.414\n",
      "Epoch 340 Batch   20/21   train_loss = 0.418\n",
      "Epoch 341 Batch    9/21   train_loss = 0.399\n",
      "Epoch 341 Batch   19/21   train_loss = 0.377\n",
      "Epoch 342 Batch    8/21   train_loss = 0.407\n",
      "Epoch 342 Batch   18/21   train_loss = 0.399\n",
      "Epoch 343 Batch    7/21   train_loss = 0.372\n",
      "Epoch 343 Batch   17/21   train_loss = 0.407\n",
      "Epoch 344 Batch    6/21   train_loss = 0.380\n",
      "Epoch 344 Batch   16/21   train_loss = 0.349\n",
      "Epoch 345 Batch    5/21   train_loss = 0.405\n",
      "Epoch 345 Batch   15/21   train_loss = 0.405\n",
      "Epoch 346 Batch    4/21   train_loss = 0.409\n",
      "Epoch 346 Batch   14/21   train_loss = 0.427\n",
      "Epoch 347 Batch    3/21   train_loss = 0.425\n",
      "Epoch 347 Batch   13/21   train_loss = 0.438\n",
      "Epoch 348 Batch    2/21   train_loss = 0.415\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 348 Batch   12/21   train_loss = 0.382\n",
      "Epoch 349 Batch    1/21   train_loss = 0.403\n",
      "Epoch 349 Batch   11/21   train_loss = 0.396\n",
      "Epoch 350 Batch    0/21   train_loss = 0.370\n",
      "Epoch 350 Batch   10/21   train_loss = 0.418\n",
      "Epoch 350 Batch   20/21   train_loss = 0.435\n",
      "Epoch 351 Batch    9/21   train_loss = 0.443\n",
      "Epoch 351 Batch   19/21   train_loss = 0.437\n",
      "Epoch 352 Batch    8/21   train_loss = 0.468\n",
      "Epoch 352 Batch   18/21   train_loss = 0.460\n",
      "Epoch 353 Batch    7/21   train_loss = 0.426\n",
      "Epoch 353 Batch   17/21   train_loss = 0.446\n",
      "Epoch 354 Batch    6/21   train_loss = 0.400\n",
      "Epoch 354 Batch   16/21   train_loss = 0.365\n",
      "Epoch 355 Batch    5/21   train_loss = 0.404\n",
      "Epoch 355 Batch   15/21   train_loss = 0.387\n",
      "Epoch 356 Batch    4/21   train_loss = 0.363\n",
      "Epoch 356 Batch   14/21   train_loss = 0.366\n",
      "Epoch 357 Batch    3/21   train_loss = 0.382\n",
      "Epoch 357 Batch   13/21   train_loss = 0.390\n",
      "Epoch 358 Batch    2/21   train_loss = 0.369\n",
      "Epoch 358 Batch   12/21   train_loss = 0.354\n",
      "Epoch 359 Batch    1/21   train_loss = 0.360\n",
      "Epoch 359 Batch   11/21   train_loss = 0.357\n",
      "Epoch 360 Batch    0/21   train_loss = 0.326\n",
      "Epoch 360 Batch   10/21   train_loss = 0.357\n",
      "Epoch 360 Batch   20/21   train_loss = 0.357\n",
      "Epoch 361 Batch    9/21   train_loss = 0.341\n",
      "Epoch 361 Batch   19/21   train_loss = 0.325\n",
      "Epoch 362 Batch    8/21   train_loss = 0.356\n",
      "Epoch 362 Batch   18/21   train_loss = 0.353\n",
      "Epoch 363 Batch    7/21   train_loss = 0.328\n",
      "Epoch 363 Batch   17/21   train_loss = 0.360\n",
      "Epoch 364 Batch    6/21   train_loss = 0.326\n",
      "Epoch 364 Batch   16/21   train_loss = 0.299\n",
      "Epoch 365 Batch    5/21   train_loss = 0.334\n",
      "Epoch 365 Batch   15/21   train_loss = 0.326\n",
      "Epoch 366 Batch    4/21   train_loss = 0.314\n",
      "Epoch 366 Batch   14/21   train_loss = 0.316\n",
      "Epoch 367 Batch    3/21   train_loss = 0.332\n",
      "Epoch 367 Batch   13/21   train_loss = 0.334\n",
      "Epoch 368 Batch    2/21   train_loss = 0.320\n",
      "Epoch 368 Batch   12/21   train_loss = 0.307\n",
      "Epoch 369 Batch    1/21   train_loss = 0.321\n",
      "Epoch 369 Batch   11/21   train_loss = 0.326\n",
      "Epoch 370 Batch    0/21   train_loss = 0.298\n",
      "Epoch 370 Batch   10/21   train_loss = 0.334\n",
      "Epoch 370 Batch   20/21   train_loss = 0.334\n",
      "Epoch 371 Batch    9/21   train_loss = 0.324\n",
      "Epoch 371 Batch   19/21   train_loss = 0.311\n",
      "Epoch 372 Batch    8/21   train_loss = 0.343\n",
      "Epoch 372 Batch   18/21   train_loss = 0.343\n",
      "Epoch 373 Batch    7/21   train_loss = 0.321\n",
      "Epoch 373 Batch   17/21   train_loss = 0.354\n",
      "Epoch 374 Batch    6/21   train_loss = 0.316\n",
      "Epoch 374 Batch   16/21   train_loss = 0.290\n",
      "Epoch 375 Batch    5/21   train_loss = 0.323\n",
      "Epoch 375 Batch   15/21   train_loss = 0.312\n",
      "Epoch 376 Batch    4/21   train_loss = 0.299\n",
      "Epoch 376 Batch   14/21   train_loss = 0.299\n",
      "Epoch 377 Batch    3/21   train_loss = 0.315\n",
      "Epoch 377 Batch   13/21   train_loss = 0.319\n",
      "Epoch 378 Batch    2/21   train_loss = 0.308\n",
      "Epoch 378 Batch   12/21   train_loss = 0.301\n",
      "Epoch 379 Batch    1/21   train_loss = 0.317\n",
      "Epoch 379 Batch   11/21   train_loss = 0.328\n",
      "Epoch 380 Batch    0/21   train_loss = 0.300\n",
      "Epoch 380 Batch   10/21   train_loss = 0.340\n",
      "Epoch 380 Batch   20/21   train_loss = 0.341\n",
      "Epoch 381 Batch    9/21   train_loss = 0.329\n",
      "Epoch 381 Batch   19/21   train_loss = 0.311\n",
      "Epoch 382 Batch    8/21   train_loss = 0.340\n",
      "Epoch 382 Batch   18/21   train_loss = 0.333\n",
      "Epoch 383 Batch    7/21   train_loss = 0.308\n",
      "Epoch 383 Batch   17/21   train_loss = 0.338\n",
      "Epoch 384 Batch    6/21   train_loss = 0.302\n",
      "Epoch 384 Batch   16/21   train_loss = 0.276\n",
      "Epoch 385 Batch    5/21   train_loss = 0.314\n",
      "Epoch 385 Batch   15/21   train_loss = 0.313\n",
      "Epoch 386 Batch    4/21   train_loss = 0.302\n",
      "Epoch 386 Batch   14/21   train_loss = 0.319\n",
      "Epoch 387 Batch    3/21   train_loss = 0.338\n",
      "Epoch 387 Batch   13/21   train_loss = 0.344\n",
      "Epoch 388 Batch    2/21   train_loss = 0.335\n",
      "Epoch 388 Batch   12/21   train_loss = 0.327\n",
      "Epoch 389 Batch    1/21   train_loss = 0.336\n",
      "Epoch 389 Batch   11/21   train_loss = 0.334\n",
      "Epoch 390 Batch    0/21   train_loss = 0.294\n",
      "Epoch 390 Batch   10/21   train_loss = 0.336\n",
      "Epoch 390 Batch   20/21   train_loss = 0.323\n",
      "Epoch 391 Batch    9/21   train_loss = 0.315\n",
      "Epoch 391 Batch   19/21   train_loss = 0.304\n",
      "Epoch 392 Batch    8/21   train_loss = 0.340\n",
      "Epoch 392 Batch   18/21   train_loss = 0.361\n",
      "Epoch 393 Batch    7/21   train_loss = 0.350\n",
      "Epoch 393 Batch   17/21   train_loss = 0.388\n",
      "Epoch 394 Batch    6/21   train_loss = 0.360\n",
      "Epoch 394 Batch   16/21   train_loss = 0.337\n",
      "Epoch 395 Batch    5/21   train_loss = 0.358\n",
      "Epoch 395 Batch   15/21   train_loss = 0.337\n",
      "Epoch 396 Batch    4/21   train_loss = 0.307\n",
      "Epoch 396 Batch   14/21   train_loss = 0.306\n",
      "Epoch 397 Batch    3/21   train_loss = 0.328\n",
      "Epoch 397 Batch   13/21   train_loss = 0.344\n",
      "Epoch 398 Batch    2/21   train_loss = 0.356\n",
      "Epoch 398 Batch   12/21   train_loss = 0.363\n",
      "Epoch 399 Batch    1/21   train_loss = 0.392\n",
      "Epoch 399 Batch   11/21   train_loss = 0.375\n",
      "Epoch 400 Batch    0/21   train_loss = 0.345\n",
      "Epoch 400 Batch   10/21   train_loss = 0.361\n",
      "Epoch 400 Batch   20/21   train_loss = 0.344\n",
      "Epoch 401 Batch    9/21   train_loss = 0.334\n",
      "Epoch 401 Batch   19/21   train_loss = 0.319\n",
      "Epoch 402 Batch    8/21   train_loss = 0.359\n",
      "Epoch 402 Batch   18/21   train_loss = 0.345\n",
      "Epoch 403 Batch    7/21   train_loss = 0.318\n",
      "Epoch 403 Batch   17/21   train_loss = 0.340\n",
      "Epoch 404 Batch    6/21   train_loss = 0.297\n",
      "Epoch 404 Batch   16/21   train_loss = 0.276\n",
      "Epoch 405 Batch    5/21   train_loss = 0.300\n",
      "Epoch 405 Batch   15/21   train_loss = 0.290\n",
      "Epoch 406 Batch    4/21   train_loss = 0.273\n",
      "Epoch 406 Batch   14/21   train_loss = 0.276\n",
      "Epoch 407 Batch    3/21   train_loss = 0.292\n",
      "Epoch 407 Batch   13/21   train_loss = 0.294\n",
      "Epoch 408 Batch    2/21   train_loss = 0.282\n",
      "Epoch 408 Batch   12/21   train_loss = 0.273\n",
      "Epoch 409 Batch    1/21   train_loss = 0.282\n",
      "Epoch 409 Batch   11/21   train_loss = 0.289\n",
      "Epoch 410 Batch    0/21   train_loss = 0.263\n",
      "Epoch 410 Batch   10/21   train_loss = 0.296\n",
      "Epoch 410 Batch   20/21   train_loss = 0.290\n",
      "Epoch 411 Batch    9/21   train_loss = 0.281\n",
      "Epoch 411 Batch   19/21   train_loss = 0.268\n",
      "Epoch 412 Batch    8/21   train_loss = 0.296\n",
      "Epoch 412 Batch   18/21   train_loss = 0.292\n",
      "Epoch 413 Batch    7/21   train_loss = 0.271\n",
      "Epoch 413 Batch   17/21   train_loss = 0.305\n",
      "Epoch 414 Batch    6/21   train_loss = 0.265\n",
      "Epoch 414 Batch   16/21   train_loss = 0.246\n",
      "Epoch 415 Batch    5/21   train_loss = 0.277\n",
      "Epoch 415 Batch   15/21   train_loss = 0.273\n",
      "Epoch 416 Batch    4/21   train_loss = 0.257\n",
      "Epoch 416 Batch   14/21   train_loss = 0.262\n",
      "Epoch 417 Batch    3/21   train_loss = 0.278\n",
      "Epoch 417 Batch   13/21   train_loss = 0.281\n",
      "Epoch 418 Batch    2/21   train_loss = 0.271\n",
      "Epoch 418 Batch   12/21   train_loss = 0.262\n",
      "Epoch 419 Batch    1/21   train_loss = 0.270\n",
      "Epoch 419 Batch   11/21   train_loss = 0.279\n",
      "Epoch 420 Batch    0/21   train_loss = 0.253\n",
      "Epoch 420 Batch   10/21   train_loss = 0.287\n",
      "Epoch 420 Batch   20/21   train_loss = 0.280\n",
      "Epoch 421 Batch    9/21   train_loss = 0.272\n",
      "Epoch 421 Batch   19/21   train_loss = 0.259\n",
      "Epoch 422 Batch    8/21   train_loss = 0.288\n",
      "Epoch 422 Batch   18/21   train_loss = 0.284\n",
      "Epoch 423 Batch    7/21   train_loss = 0.263\n",
      "Epoch 423 Batch   17/21   train_loss = 0.297\n",
      "Epoch 424 Batch    6/21   train_loss = 0.257\n",
      "Epoch 424 Batch   16/21   train_loss = 0.239\n",
      "Epoch 425 Batch    5/21   train_loss = 0.268\n",
      "Epoch 425 Batch   15/21   train_loss = 0.266\n",
      "Epoch 426 Batch    4/21   train_loss = 0.250\n",
      "Epoch 426 Batch   14/21   train_loss = 0.255\n",
      "Epoch 427 Batch    3/21   train_loss = 0.271\n",
      "Epoch 427 Batch   13/21   train_loss = 0.274\n",
      "Epoch 428 Batch    2/21   train_loss = 0.264\n",
      "Epoch 428 Batch   12/21   train_loss = 0.258\n",
      "Epoch 429 Batch    1/21   train_loss = 0.264\n",
      "Epoch 429 Batch   11/21   train_loss = 0.273\n",
      "Epoch 430 Batch    0/21   train_loss = 0.247\n",
      "Epoch 430 Batch   10/21   train_loss = 0.280\n",
      "Epoch 430 Batch   20/21   train_loss = 0.273\n",
      "Epoch 431 Batch    9/21   train_loss = 0.265\n",
      "Epoch 431 Batch   19/21   train_loss = 0.253\n",
      "Epoch 432 Batch    8/21   train_loss = 0.281\n",
      "Epoch 432 Batch   18/21   train_loss = 0.277\n",
      "Epoch 433 Batch    7/21   train_loss = 0.257\n",
      "Epoch 433 Batch   17/21   train_loss = 0.291\n",
      "Epoch 434 Batch    6/21   train_loss = 0.251\n",
      "Epoch 434 Batch   16/21   train_loss = 0.234\n",
      "Epoch 435 Batch    5/21   train_loss = 0.262\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 435 Batch   15/21   train_loss = 0.262\n",
      "Epoch 436 Batch    4/21   train_loss = 0.245\n",
      "Epoch 436 Batch   14/21   train_loss = 0.250\n",
      "Epoch 437 Batch    3/21   train_loss = 0.266\n",
      "Epoch 437 Batch   13/21   train_loss = 0.269\n",
      "Epoch 438 Batch    2/21   train_loss = 0.259\n",
      "Epoch 438 Batch   12/21   train_loss = 0.253\n",
      "Epoch 439 Batch    1/21   train_loss = 0.258\n",
      "Epoch 439 Batch   11/21   train_loss = 0.267\n",
      "Epoch 440 Batch    0/21   train_loss = 0.242\n",
      "Epoch 440 Batch   10/21   train_loss = 0.275\n",
      "Epoch 440 Batch   20/21   train_loss = 0.268\n",
      "Epoch 441 Batch    9/21   train_loss = 0.260\n",
      "Epoch 441 Batch   19/21   train_loss = 0.248\n",
      "Epoch 442 Batch    8/21   train_loss = 0.276\n",
      "Epoch 442 Batch   18/21   train_loss = 0.271\n",
      "Epoch 443 Batch    7/21   train_loss = 0.251\n",
      "Epoch 443 Batch   17/21   train_loss = 0.286\n",
      "Epoch 444 Batch    6/21   train_loss = 0.247\n",
      "Epoch 444 Batch   16/21   train_loss = 0.230\n",
      "Epoch 445 Batch    5/21   train_loss = 0.258\n",
      "Epoch 445 Batch   15/21   train_loss = 0.258\n",
      "Epoch 446 Batch    4/21   train_loss = 0.241\n",
      "Epoch 446 Batch   14/21   train_loss = 0.245\n",
      "Epoch 447 Batch    3/21   train_loss = 0.262\n",
      "Epoch 447 Batch   13/21   train_loss = 0.265\n",
      "Epoch 448 Batch    2/21   train_loss = 0.254\n",
      "Epoch 448 Batch   12/21   train_loss = 0.249\n",
      "Epoch 449 Batch    1/21   train_loss = 0.253\n",
      "Epoch 449 Batch   11/21   train_loss = 0.261\n",
      "Epoch 450 Batch    0/21   train_loss = 0.237\n",
      "Epoch 450 Batch   10/21   train_loss = 0.271\n",
      "Epoch 450 Batch   20/21   train_loss = 0.263\n",
      "Epoch 451 Batch    9/21   train_loss = 0.256\n",
      "Epoch 451 Batch   19/21   train_loss = 0.244\n",
      "Epoch 452 Batch    8/21   train_loss = 0.272\n",
      "Epoch 452 Batch   18/21   train_loss = 0.267\n",
      "Epoch 453 Batch    7/21   train_loss = 0.247\n",
      "Epoch 453 Batch   17/21   train_loss = 0.282\n",
      "Epoch 454 Batch    6/21   train_loss = 0.243\n",
      "Epoch 454 Batch   16/21   train_loss = 0.226\n",
      "Epoch 455 Batch    5/21   train_loss = 0.254\n",
      "Epoch 455 Batch   15/21   train_loss = 0.253\n",
      "Epoch 456 Batch    4/21   train_loss = 0.237\n",
      "Epoch 456 Batch   14/21   train_loss = 0.240\n",
      "Epoch 457 Batch    3/21   train_loss = 0.257\n",
      "Epoch 457 Batch   13/21   train_loss = 0.260\n",
      "Epoch 458 Batch    2/21   train_loss = 0.250\n",
      "Epoch 458 Batch   12/21   train_loss = 0.245\n",
      "Epoch 459 Batch    1/21   train_loss = 0.248\n",
      "Epoch 459 Batch   11/21   train_loss = 0.258\n",
      "Epoch 460 Batch    0/21   train_loss = 0.234\n",
      "Epoch 460 Batch   10/21   train_loss = 0.267\n",
      "Epoch 460 Batch   20/21   train_loss = 0.259\n",
      "Epoch 461 Batch    9/21   train_loss = 0.252\n",
      "Epoch 461 Batch   19/21   train_loss = 0.241\n",
      "Epoch 462 Batch    8/21   train_loss = 0.268\n",
      "Epoch 462 Batch   18/21   train_loss = 0.263\n",
      "Epoch 463 Batch    7/21   train_loss = 0.246\n",
      "Epoch 463 Batch   17/21   train_loss = 0.280\n",
      "Epoch 464 Batch    6/21   train_loss = 0.240\n",
      "Epoch 464 Batch   16/21   train_loss = 0.224\n",
      "Epoch 465 Batch    5/21   train_loss = 0.251\n",
      "Epoch 465 Batch   15/21   train_loss = 0.250\n",
      "Epoch 466 Batch    4/21   train_loss = 0.234\n",
      "Epoch 466 Batch   14/21   train_loss = 0.237\n",
      "Epoch 467 Batch    3/21   train_loss = 0.254\n",
      "Epoch 467 Batch   13/21   train_loss = 0.256\n",
      "Epoch 468 Batch    2/21   train_loss = 0.247\n",
      "Epoch 468 Batch   12/21   train_loss = 0.243\n",
      "Epoch 469 Batch    1/21   train_loss = 0.246\n",
      "Epoch 469 Batch   11/21   train_loss = 0.255\n",
      "Epoch 470 Batch    0/21   train_loss = 0.231\n",
      "Epoch 470 Batch   10/21   train_loss = 0.266\n",
      "Epoch 470 Batch   20/21   train_loss = 0.257\n",
      "Epoch 471 Batch    9/21   train_loss = 0.249\n",
      "Epoch 471 Batch   19/21   train_loss = 0.239\n",
      "Epoch 472 Batch    8/21   train_loss = 0.266\n",
      "Epoch 472 Batch   18/21   train_loss = 0.260\n",
      "Epoch 473 Batch    7/21   train_loss = 0.241\n",
      "Epoch 473 Batch   17/21   train_loss = 0.276\n",
      "Epoch 474 Batch    6/21   train_loss = 0.236\n",
      "Epoch 474 Batch   16/21   train_loss = 0.220\n",
      "Epoch 475 Batch    5/21   train_loss = 0.247\n",
      "Epoch 475 Batch   15/21   train_loss = 0.247\n",
      "Epoch 476 Batch    4/21   train_loss = 0.229\n",
      "Epoch 476 Batch   14/21   train_loss = 0.234\n",
      "Epoch 477 Batch    3/21   train_loss = 0.251\n",
      "Epoch 477 Batch   13/21   train_loss = 0.253\n",
      "Epoch 478 Batch    2/21   train_loss = 0.245\n",
      "Epoch 478 Batch   12/21   train_loss = 0.239\n",
      "Epoch 479 Batch    1/21   train_loss = 0.244\n",
      "Epoch 479 Batch   11/21   train_loss = 0.253\n",
      "Epoch 480 Batch    0/21   train_loss = 0.230\n",
      "Epoch 480 Batch   10/21   train_loss = 0.263\n",
      "Epoch 480 Batch   20/21   train_loss = 0.255\n",
      "Epoch 481 Batch    9/21   train_loss = 0.247\n",
      "Epoch 481 Batch   19/21   train_loss = 0.237\n",
      "Epoch 482 Batch    8/21   train_loss = 0.262\n",
      "Epoch 482 Batch   18/21   train_loss = 0.257\n",
      "Epoch 483 Batch    7/21   train_loss = 0.237\n",
      "Epoch 483 Batch   17/21   train_loss = 0.273\n",
      "Epoch 484 Batch    6/21   train_loss = 0.233\n",
      "Epoch 484 Batch   16/21   train_loss = 0.218\n",
      "Epoch 485 Batch    5/21   train_loss = 0.244\n",
      "Epoch 485 Batch   15/21   train_loss = 0.245\n",
      "Epoch 486 Batch    4/21   train_loss = 0.228\n",
      "Epoch 486 Batch   14/21   train_loss = 0.234\n",
      "Epoch 487 Batch    3/21   train_loss = 0.251\n",
      "Epoch 487 Batch   13/21   train_loss = 0.253\n",
      "Epoch 488 Batch    2/21   train_loss = 0.246\n",
      "Epoch 488 Batch   12/21   train_loss = 0.239\n",
      "Epoch 489 Batch    1/21   train_loss = 0.244\n",
      "Epoch 489 Batch   11/21   train_loss = 0.252\n",
      "Epoch 490 Batch    0/21   train_loss = 0.227\n",
      "Epoch 490 Batch   10/21   train_loss = 0.262\n",
      "Epoch 490 Batch   20/21   train_loss = 0.253\n",
      "Epoch 491 Batch    9/21   train_loss = 0.244\n",
      "Epoch 491 Batch   19/21   train_loss = 0.234\n",
      "Epoch 492 Batch    8/21   train_loss = 0.261\n",
      "Epoch 492 Batch   18/21   train_loss = 0.256\n",
      "Epoch 493 Batch    7/21   train_loss = 0.237\n",
      "Epoch 493 Batch   17/21   train_loss = 0.273\n",
      "Epoch 494 Batch    6/21   train_loss = 0.233\n",
      "Epoch 494 Batch   16/21   train_loss = 0.222\n",
      "Epoch 495 Batch    5/21   train_loss = 0.248\n",
      "Epoch 495 Batch   15/21   train_loss = 0.247\n",
      "Epoch 496 Batch    4/21   train_loss = 0.232\n",
      "Epoch 496 Batch   14/21   train_loss = 0.235\n",
      "Epoch 497 Batch    3/21   train_loss = 0.252\n",
      "Epoch 497 Batch   13/21   train_loss = 0.258\n",
      "Epoch 498 Batch    2/21   train_loss = 0.247\n",
      "Epoch 498 Batch   12/21   train_loss = 0.239\n",
      "Epoch 499 Batch    1/21   train_loss = 0.244\n",
      "Epoch 499 Batch   11/21   train_loss = 0.251\n",
      "Epoch 500 Batch    0/21   train_loss = 0.226\n",
      "Epoch 500 Batch   10/21   train_loss = 0.260\n",
      "Epoch 500 Batch   20/21   train_loss = 0.251\n",
      "Epoch 501 Batch    9/21   train_loss = 0.244\n",
      "Epoch 501 Batch   19/21   train_loss = 0.232\n",
      "Epoch 502 Batch    8/21   train_loss = 0.262\n",
      "Epoch 502 Batch   18/21   train_loss = 0.259\n",
      "Epoch 503 Batch    7/21   train_loss = 0.243\n",
      "Epoch 503 Batch   17/21   train_loss = 0.276\n",
      "Epoch 504 Batch    6/21   train_loss = 0.237\n",
      "Epoch 504 Batch   16/21   train_loss = 0.226\n",
      "Epoch 505 Batch    5/21   train_loss = 0.248\n",
      "Epoch 505 Batch   15/21   train_loss = 0.249\n",
      "Epoch 506 Batch    4/21   train_loss = 0.235\n",
      "Epoch 506 Batch   14/21   train_loss = 0.235\n",
      "Epoch 507 Batch    3/21   train_loss = 0.251\n",
      "Epoch 507 Batch   13/21   train_loss = 0.254\n",
      "Epoch 508 Batch    2/21   train_loss = 0.244\n",
      "Epoch 508 Batch   12/21   train_loss = 0.236\n",
      "Epoch 509 Batch    1/21   train_loss = 0.241\n",
      "Epoch 509 Batch   11/21   train_loss = 0.248\n",
      "Epoch 510 Batch    0/21   train_loss = 0.230\n",
      "Epoch 510 Batch   10/21   train_loss = 0.261\n",
      "Epoch 510 Batch   20/21   train_loss = 0.255\n",
      "Epoch 511 Batch    9/21   train_loss = 0.257\n",
      "Epoch 511 Batch   19/21   train_loss = 0.239\n",
      "Epoch 512 Batch    8/21   train_loss = 0.269\n",
      "Epoch 512 Batch   18/21   train_loss = 0.274\n",
      "Epoch 513 Batch    7/21   train_loss = 0.259\n",
      "Epoch 513 Batch   17/21   train_loss = 0.287\n",
      "Epoch 514 Batch    6/21   train_loss = 0.242\n",
      "Epoch 514 Batch   16/21   train_loss = 0.239\n",
      "Epoch 515 Batch    5/21   train_loss = 0.256\n",
      "Epoch 515 Batch   15/21   train_loss = 0.251\n",
      "Epoch 516 Batch    4/21   train_loss = 0.236\n",
      "Epoch 516 Batch   14/21   train_loss = 0.236\n",
      "Epoch 517 Batch    3/21   train_loss = 0.257\n",
      "Epoch 517 Batch   13/21   train_loss = 0.257\n",
      "Epoch 518 Batch    2/21   train_loss = 0.251\n",
      "Epoch 518 Batch   12/21   train_loss = 0.256\n",
      "Epoch 519 Batch    1/21   train_loss = 0.259\n",
      "Epoch 519 Batch   11/21   train_loss = 0.273\n",
      "Epoch 520 Batch    0/21   train_loss = 0.266\n",
      "Epoch 520 Batch   10/21   train_loss = 0.290\n",
      "Epoch 520 Batch   20/21   train_loss = 0.285\n",
      "Epoch 521 Batch    9/21   train_loss = 0.279\n",
      "Epoch 521 Batch   19/21   train_loss = 0.267\n",
      "Epoch 522 Batch    8/21   train_loss = 0.286\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 522 Batch   18/21   train_loss = 0.280\n",
      "Epoch 523 Batch    7/21   train_loss = 0.264\n",
      "Epoch 523 Batch   17/21   train_loss = 0.293\n",
      "Epoch 524 Batch    6/21   train_loss = 0.254\n",
      "Epoch 524 Batch   16/21   train_loss = 0.243\n",
      "Epoch 525 Batch    5/21   train_loss = 0.281\n",
      "Epoch 525 Batch   15/21   train_loss = 0.285\n",
      "Epoch 526 Batch    4/21   train_loss = 0.284\n",
      "Epoch 526 Batch   14/21   train_loss = 0.287\n",
      "Epoch 527 Batch    3/21   train_loss = 0.302\n",
      "Epoch 527 Batch   13/21   train_loss = 0.308\n",
      "Epoch 528 Batch    2/21   train_loss = 0.301\n",
      "Epoch 528 Batch   12/21   train_loss = 0.284\n",
      "Epoch 529 Batch    1/21   train_loss = 0.284\n",
      "Epoch 529 Batch   11/21   train_loss = 0.280\n",
      "Epoch 530 Batch    0/21   train_loss = 0.253\n",
      "Epoch 530 Batch   10/21   train_loss = 0.290\n",
      "Epoch 530 Batch   20/21   train_loss = 0.285\n",
      "Epoch 531 Batch    9/21   train_loss = 0.278\n",
      "Epoch 531 Batch   19/21   train_loss = 0.275\n",
      "Epoch 532 Batch    8/21   train_loss = 0.300\n",
      "Epoch 532 Batch   18/21   train_loss = 0.306\n",
      "Epoch 533 Batch    7/21   train_loss = 0.280\n",
      "Epoch 533 Batch   17/21   train_loss = 0.314\n",
      "Epoch 534 Batch    6/21   train_loss = 0.271\n",
      "Epoch 534 Batch   16/21   train_loss = 0.246\n",
      "Epoch 535 Batch    5/21   train_loss = 0.263\n",
      "Epoch 535 Batch   15/21   train_loss = 0.257\n",
      "Epoch 536 Batch    4/21   train_loss = 0.239\n",
      "Epoch 536 Batch   14/21   train_loss = 0.241\n",
      "Epoch 537 Batch    3/21   train_loss = 0.268\n",
      "Epoch 537 Batch   13/21   train_loss = 0.260\n",
      "Epoch 538 Batch    2/21   train_loss = 0.242\n",
      "Epoch 538 Batch   12/21   train_loss = 0.234\n",
      "Epoch 539 Batch    1/21   train_loss = 0.235\n",
      "Epoch 539 Batch   11/21   train_loss = 0.242\n",
      "Epoch 540 Batch    0/21   train_loss = 0.220\n",
      "Epoch 540 Batch   10/21   train_loss = 0.253\n",
      "Epoch 540 Batch   20/21   train_loss = 0.242\n",
      "Epoch 541 Batch    9/21   train_loss = 0.234\n",
      "Epoch 541 Batch   19/21   train_loss = 0.223\n",
      "Epoch 542 Batch    8/21   train_loss = 0.249\n",
      "Epoch 542 Batch   18/21   train_loss = 0.243\n",
      "Epoch 543 Batch    7/21   train_loss = 0.225\n",
      "Epoch 543 Batch   17/21   train_loss = 0.259\n",
      "Epoch 544 Batch    6/21   train_loss = 0.219\n",
      "Epoch 544 Batch   16/21   train_loss = 0.206\n",
      "Epoch 545 Batch    5/21   train_loss = 0.231\n",
      "Epoch 545 Batch   15/21   train_loss = 0.233\n",
      "Epoch 546 Batch    4/21   train_loss = 0.215\n",
      "Epoch 546 Batch   14/21   train_loss = 0.219\n",
      "Epoch 547 Batch    3/21   train_loss = 0.237\n",
      "Epoch 547 Batch   13/21   train_loss = 0.237\n",
      "Epoch 548 Batch    2/21   train_loss = 0.229\n",
      "Epoch 548 Batch   12/21   train_loss = 0.226\n",
      "Epoch 549 Batch    1/21   train_loss = 0.227\n",
      "Epoch 549 Batch   11/21   train_loss = 0.235\n",
      "Epoch 550 Batch    0/21   train_loss = 0.214\n",
      "Epoch 550 Batch   10/21   train_loss = 0.247\n",
      "Epoch 550 Batch   20/21   train_loss = 0.237\n",
      "Epoch 551 Batch    9/21   train_loss = 0.229\n",
      "Epoch 551 Batch   19/21   train_loss = 0.219\n",
      "Epoch 552 Batch    8/21   train_loss = 0.246\n",
      "Epoch 552 Batch   18/21   train_loss = 0.240\n",
      "Epoch 553 Batch    7/21   train_loss = 0.222\n",
      "Epoch 553 Batch   17/21   train_loss = 0.256\n",
      "Epoch 554 Batch    6/21   train_loss = 0.217\n",
      "Epoch 554 Batch   16/21   train_loss = 0.204\n",
      "Epoch 555 Batch    5/21   train_loss = 0.229\n",
      "Epoch 555 Batch   15/21   train_loss = 0.231\n",
      "Epoch 556 Batch    4/21   train_loss = 0.213\n",
      "Epoch 556 Batch   14/21   train_loss = 0.217\n",
      "Epoch 557 Batch    3/21   train_loss = 0.235\n",
      "Epoch 557 Batch   13/21   train_loss = 0.235\n",
      "Epoch 558 Batch    2/21   train_loss = 0.227\n",
      "Epoch 558 Batch   12/21   train_loss = 0.224\n",
      "Epoch 559 Batch    1/21   train_loss = 0.225\n",
      "Epoch 559 Batch   11/21   train_loss = 0.233\n",
      "Epoch 560 Batch    0/21   train_loss = 0.212\n",
      "Epoch 560 Batch   10/21   train_loss = 0.245\n",
      "Epoch 560 Batch   20/21   train_loss = 0.235\n",
      "Epoch 561 Batch    9/21   train_loss = 0.228\n",
      "Epoch 561 Batch   19/21   train_loss = 0.217\n",
      "Epoch 562 Batch    8/21   train_loss = 0.244\n",
      "Epoch 562 Batch   18/21   train_loss = 0.238\n",
      "Epoch 563 Batch    7/21   train_loss = 0.221\n",
      "Epoch 563 Batch   17/21   train_loss = 0.255\n",
      "Epoch 564 Batch    6/21   train_loss = 0.215\n",
      "Epoch 564 Batch   16/21   train_loss = 0.203\n",
      "Epoch 565 Batch    5/21   train_loss = 0.227\n",
      "Epoch 565 Batch   15/21   train_loss = 0.230\n",
      "Epoch 566 Batch    4/21   train_loss = 0.212\n",
      "Epoch 566 Batch   14/21   train_loss = 0.215\n",
      "Epoch 567 Batch    3/21   train_loss = 0.234\n",
      "Epoch 567 Batch   13/21   train_loss = 0.234\n",
      "Epoch 568 Batch    2/21   train_loss = 0.226\n",
      "Epoch 568 Batch   12/21   train_loss = 0.223\n",
      "Epoch 569 Batch    1/21   train_loss = 0.223\n",
      "Epoch 569 Batch   11/21   train_loss = 0.232\n",
      "Epoch 570 Batch    0/21   train_loss = 0.211\n",
      "Epoch 570 Batch   10/21   train_loss = 0.244\n",
      "Epoch 570 Batch   20/21   train_loss = 0.233\n",
      "Epoch 571 Batch    9/21   train_loss = 0.226\n",
      "Epoch 571 Batch   19/21   train_loss = 0.216\n",
      "Epoch 572 Batch    8/21   train_loss = 0.243\n",
      "Epoch 572 Batch   18/21   train_loss = 0.237\n",
      "Epoch 573 Batch    7/21   train_loss = 0.219\n",
      "Epoch 573 Batch   17/21   train_loss = 0.254\n",
      "Epoch 574 Batch    6/21   train_loss = 0.214\n",
      "Epoch 574 Batch   16/21   train_loss = 0.202\n",
      "Epoch 575 Batch    5/21   train_loss = 0.226\n",
      "Epoch 575 Batch   15/21   train_loss = 0.229\n",
      "Epoch 576 Batch    4/21   train_loss = 0.211\n",
      "Epoch 576 Batch   14/21   train_loss = 0.214\n",
      "Epoch 577 Batch    3/21   train_loss = 0.232\n",
      "Epoch 577 Batch   13/21   train_loss = 0.233\n",
      "Epoch 578 Batch    2/21   train_loss = 0.225\n",
      "Epoch 578 Batch   12/21   train_loss = 0.222\n",
      "Epoch 579 Batch    1/21   train_loss = 0.222\n",
      "Epoch 579 Batch   11/21   train_loss = 0.231\n",
      "Epoch 580 Batch    0/21   train_loss = 0.210\n",
      "Epoch 580 Batch   10/21   train_loss = 0.243\n",
      "Epoch 580 Batch   20/21   train_loss = 0.232\n",
      "Epoch 581 Batch    9/21   train_loss = 0.226\n",
      "Epoch 581 Batch   19/21   train_loss = 0.215\n",
      "Epoch 582 Batch    8/21   train_loss = 0.242\n",
      "Epoch 582 Batch   18/21   train_loss = 0.236\n",
      "Epoch 583 Batch    7/21   train_loss = 0.218\n",
      "Epoch 583 Batch   17/21   train_loss = 0.253\n",
      "Epoch 584 Batch    6/21   train_loss = 0.213\n",
      "Epoch 584 Batch   16/21   train_loss = 0.201\n",
      "Epoch 585 Batch    5/21   train_loss = 0.225\n",
      "Epoch 585 Batch   15/21   train_loss = 0.228\n",
      "Epoch 586 Batch    4/21   train_loss = 0.210\n",
      "Epoch 586 Batch   14/21   train_loss = 0.213\n",
      "Epoch 587 Batch    3/21   train_loss = 0.232\n",
      "Epoch 587 Batch   13/21   train_loss = 0.232\n",
      "Epoch 588 Batch    2/21   train_loss = 0.224\n",
      "Epoch 588 Batch   12/21   train_loss = 0.222\n",
      "Epoch 589 Batch    1/21   train_loss = 0.222\n",
      "Epoch 589 Batch   11/21   train_loss = 0.230\n",
      "Epoch 590 Batch    0/21   train_loss = 0.209\n",
      "Epoch 590 Batch   10/21   train_loss = 0.242\n",
      "Epoch 590 Batch   20/21   train_loss = 0.232\n",
      "Epoch 591 Batch    9/21   train_loss = 0.225\n",
      "Epoch 591 Batch   19/21   train_loss = 0.214\n",
      "Epoch 592 Batch    8/21   train_loss = 0.241\n",
      "Epoch 592 Batch   18/21   train_loss = 0.236\n",
      "Epoch 593 Batch    7/21   train_loss = 0.218\n",
      "Epoch 593 Batch   17/21   train_loss = 0.252\n",
      "Epoch 594 Batch    6/21   train_loss = 0.213\n",
      "Epoch 594 Batch   16/21   train_loss = 0.200\n",
      "Epoch 595 Batch    5/21   train_loss = 0.225\n",
      "Epoch 595 Batch   15/21   train_loss = 0.227\n",
      "Epoch 596 Batch    4/21   train_loss = 0.209\n",
      "Epoch 596 Batch   14/21   train_loss = 0.212\n",
      "Epoch 597 Batch    3/21   train_loss = 0.231\n",
      "Epoch 597 Batch   13/21   train_loss = 0.232\n",
      "Epoch 598 Batch    2/21   train_loss = 0.223\n",
      "Epoch 598 Batch   12/21   train_loss = 0.221\n",
      "Epoch 599 Batch    1/21   train_loss = 0.221\n",
      "Epoch 599 Batch   11/21   train_loss = 0.229\n",
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batches = get_batches(int_text, batch_size, seq_length)\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Run for 1000 epochs with learning rate equal to 0.001\n",
    "    for epoch_i in range(num_epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "\n",
    "        for batch_i, (x, y) in enumerate(batches):\n",
    "            feed = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                lr: 0.001}\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed)\n",
    "\n",
    "            # Show every <show_every_n_batches> batches\n",
    "            if (epoch_i * len(batches) + batch_i) % show_every_n_batches == 0:\n",
    "                print('Epoch {:>3} Batch {:>4}/{}   train_loss = {:.3f}'.format(\n",
    "                    epoch_i,\n",
    "                    batch_i,\n",
    "                    len(batches),\n",
    "                    train_loss))    \n",
    "\n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_dir)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate TV Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./save\n",
      "peter: i was gonna call them. but my favorite episode of different strokes was on. tomorrow.\n",
      "lois: yeah. i have no son, i know? i've fooled people before. you're fired!\n",
      "peter: yeah! i wouldn't have to the right. it's the griffins.\n",
      "lois: stop it.\n",
      "brian: oh, god, that was good. i just had a peter griffin production. hey, i can't leave more important, and that's final. see, i'm peter griffin.\n",
      "peter: yeah, it's okay. i was just a formality, since i happen to be just like when we can start with someone else being worshipped like him.\n",
      "brian: peter, brian.\n",
      "brian: justice. what the hell?\n",
      "brian: you don't know what love's like.\n",
      "stewie: oh, you! stay![ dramatic instrumental music]\n",
      "\n",
      "peter:[ nervous laughter]\n",
      "\n",
      "lois: we didn't measure!\n",
      "peter: wait, here, you guys.\n",
      "chris: oh, dad, you did my whole day's work have a job, peter?\n",
      "peter: we're being chased by ghosts!\n",
      "\n",
      "death: peter, what the hell is that? i'm not! you all right?\n",
      "lois: peter, what are you doing?\n",
      "lois: no!\n",
      "peter: what did you make this?\n",
      "brian: uh, no, thank you. i'm gonna go out.\n",
      "peter: brian, i do a good thing in the other room?\n",
      "stewie: why don't you burn in the pool with the family i don't think you think they wrote!\n",
      "peter: sorry, honey. i'm not a big party tonight. and part of growing up means you're right.\n",
      "lois: are you sure, honey?\n",
      "stewie: yes, you can be here in the mud. look, it's on the deck of here, kids.\n",
      "brian: come on, lois. i hate the yankees.\n",
      "lois: i'm just a little overboard. you know, it varies, you think there's a whole family.\n",
      "lois: peter, there's no reason i was like a guy in there. and we can try.\n",
      "death: all right, now go on, lois. for god's sake, i'm gonna go to that twinkie factory, and that's what i'm sure.\n",
      "\n",
      "lois: so, he's bothering everyone. say something, take pictures so.\n",
      "peter: hey, brian. what do you want?\n",
      "peter: nothin'. i'm just a little.\n",
      "guy: what a night. i know how to do you think i'm an indian. but why don't you explain it up?\n",
      "\n",
      "peter: i'll trade you guys at the other room?\n",
      "stewie: is the doggie going back to the golf course. i've got a date.\n",
      "peter: meg, honey.\n",
      "meg: i got it! as soon as that show came on tv.\n",
      "lois: it's just a little phone number, all you gotta do is tell a boy of.\n",
      "peter: don't worry. once, chris. i need to learn how to say, brian. i know you can't understand this?\n",
      "lois: no.\n",
      "quagmire: hey, petey!\n",
      "peter: jeez. how can you afford that?\n",
      "\n",
      "meg: i can't believe you'd be amazed how little you guys. i just want to eat him?\n",
      "lois: is gonna?\n",
      "chris: i can't do it!\n",
      "\n",
      "peter: hey, brian, you're embarrassing. i have to admit, this is gonna be great. cruises are not the president of a...\n",
      "lois: i should go. we'll get the one straight cop was just like there was.\n",
      "lois: oh, is that, peter?\n",
      "peter: you're getting kind of fat. oh, my god! you know i spoil you! the government is here?\n",
      "chris: i can't. i just got bounced by the pope's road ahead. you'll know what else they have had to get in touch with all over. i'm sorry. we gotta find a way to get money with all over again. you got something. you know, like some pancakes love you guys at the end of the world, i probably got a good look.\n",
      "lois: you sold our very own.\n",
      "lois: oh, my god! i found a lump!\n",
      "\n",
      "lois: this is it wonderful, honey?\n",
      "peter: hey, lois, look! hey, hey, dad. i'm also getting a boat.\n",
      "peter: hey, what was that? oh, god! i found a little...\n",
      "brian: oh, oh, god. i just had the craziest dream where are we want to do some volunteer work?\n",
      "peter: look at that. can't you two go, and i don't know when i was hoping you could give me the lower.\n",
      "stewie: well, well, then....\n",
      "loretta: mmm on, lois. i'll get you out of here!\n",
      "carter: no, seriously.\n",
      "peter: that's easy. but we're going to fenway!\n",
      "peter: what?\n",
      "tree 1: hot out for what did you call? what?\n",
      "peter: hey, brian. those little be easier on the deck of your house, too.\n",
      "\n",
      "brian: oh, come on, brian. i hate to see you something.\n",
      "death: no. one last question. if you were right, cleveland and quagmire are over.\n",
      "lois: peter, you're behind all over.\n",
      "brian: peter, i'll do it.\n",
      "peter: i'll tell you, lois' dad was a pain in my ass!\n",
      "\n",
      "lois: i guess there's nothing wrong with me.\n",
      "peter: honey, i can't hug you. hey, what are you wearing? wow, i know you're hurting. but i don't want to have to be a.\n",
      "monkey 4: yeah, lois. i'm going to hell out there and no son, uh, what the hell is this?\n",
      "lois: thursday.\n",
      "peter: i can't imagine how screwed up your heart to go to plan b.\n",
      "chris:[ growling] oh, god! oh, hi, mom. god, no!\n",
      "\n",
      "lois: brian, what are you doing here?\n",
      "chris: sorry, chris.\n",
      "brian: justice. it's a pleasure..\n",
      "lois: brian, i'm a little worried about peter. last night i was stealing joe's ladder so i could do it.\n",
      "peter: i know you can't understand what i did was wrong. i don't have a ride.\n",
      "peter: hey, john, the fat man 1: hot for a week.\n",
      "peter: yeah, i'm guessing a cop may have had something to do with all those things that changes people.\n",
      "meg: chris, do you want to try out for?\n",
      "peter: how could you afford that?\n",
      "peter: i just got bounced by the pope's road test, you know, where you were, peter. and i'm real out of the.\n",
      "\n",
      "stewie: stupid, greedy savages. okay, honey?\n",
      "death: no. one of our very good reason.\n",
      "brian: thank you, but you just need to find something. i mean, he's the spokesman for his entire industry. sometimes it's okay up with me.\n",
      "\n",
      "lois: peter, this is the speed machine[ makes the world girl over there. omnipotence! got to take your damn watch for the world, i help!\n",
      "\n",
      "lois: i should go. we'll get a girl.\n",
      "meg: what do you think.\n",
      "stewie: so i do, i won't let you buy it looks like.\n",
      "peter: oh, come on, brian. i hate the yankees.\n",
      "\n",
      "lois: i don't know, peter. lips are one thing. but did your therapist say?\n",
      "peter:[ nervous laughter]\n",
      "brian: i can't imagine how screwed up your kids?\n",
      "chris: sorry, chris. we're leaving.\n",
      "chris: yeah. i just had the craziest dream where are you upset because you went wee-wee on the.\n",
      "peter: i know you can't understand what you've learned. kathy, get in the middle of a room in time. i shall put us seriously, stewie back where this day.\n",
      "peter: hey, lois, look, pal and quagmire.\n",
      "peter: sorry, chris. don't talk like that.\n",
      "peter: i hope the boss. all right, that's okay, senator. this is where you belong. and the hell have you been?.\n",
      "stewie: cheer up, mother. couldn't we?\n",
      "brian: oh, my god. brian, please. my entire life depends on getting my pupil![ screaming]\n",
      "stewie: yes, charming. one for three.[ laughing]\n",
      "lois: what's going on, big?\n",
      "stewie: you won't believe it, mom. look, i got a date with my female years...\n",
      "death: no, no. i don't get it. this is where babies come from.\n",
      "death: why you...\n",
      "brian: uh, yeah. it's a whole place.\n",
      "lois: oh, thank god. i'm not going anywhere without your leash!\n",
      "peter: you mean, the pullout sofa bed...\n",
      "death: oh, my god![ music]\n",
      "cleveland: oh, no, thanks. i've got another seven years ahead of me.\n",
      "chris: dad, do you want to be the hero.\n",
      "lois: you sold our very own asian correspondent tricia takanawa's was any light at the end.\n",
      "\n",
      "joe: don't move, it! are you! what's the big rush?\n",
      "peter: all right!\n",
      "chris: what do you want? they made me in your own daughter!\n",
      "\n",
      "brian: well, that's it.\n",
      "brian: wait a second. didn't we just do this!\n",
      "peter: this is a great guy.\n",
      "peter: what? you want to swing?\n",
      "lois: i want to hear it!\n",
      "peter: whoa, whoa. hang on, hang on, lois. i'll do it!\n",
      "\n",
      "mrs. lindbergh: charles, he's going to do something stupid? you gotta tell me you can play this to meet you were all alling him again.[ sighs]\n",
      "\n",
      "miss watson: very good, he was wonderful? honey, i don't want to be the hero.\n",
      "lois: i don't know, peter.\n",
      "peter: well, i don't want to be a bother.\n",
      "peter: the shadow is in reality lamont cranston, wealthy?\n",
      "\n",
      "girl 1: like\n"
     ]
    }
   ],
   "source": [
    "helper.save_params((seq_length, save_dir))\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "seq_length, load_dir = helper.load_params()\n",
    "\n",
    "gen_length = 2000\n",
    "\n",
    "prime_word = 'peter'\n",
    "\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(load_dir + '.meta')\n",
    "    loader.restore(sess, load_dir)\n",
    "\n",
    "    # Get Tensors from loaded model\n",
    "    input_text, initial_state, final_state, probs = get_tensors(loaded_graph)\n",
    "\n",
    "    # Sentences generation setup\n",
    "    gen_sentences = [prime_word + ':']\n",
    "    prev_state = sess.run(initial_state, {input_text: np.array([[1]])})\n",
    "\n",
    "    # Generate sentences\n",
    "    for n in range(gen_length):\n",
    "\n",
    "        # Dynamic Input\n",
    "        dyn_input = [[vocab_to_int[word] for word in gen_sentences[-seq_length:]]]\n",
    "\n",
    "        dyn_seq_length = len(dyn_input[0])\n",
    "\n",
    "        # Get Prediction\n",
    "        probabilities, prev_state = sess.run(\n",
    "            [probs, final_state],\n",
    "            {input_text: dyn_input, initial_state: prev_state})\n",
    "\n",
    "        pred_word = pick_word(probabilities[0][dyn_seq_length-1], int_to_vocab)\n",
    "\n",
    "        gen_sentences.append(pred_word)\n",
    "    \n",
    "    # Remove tokens\n",
    "    tv_script = ' '.join(gen_sentences)\n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        tv_script = tv_script.replace(' ' + token.lower(), key)\n",
    "    tv_script = tv_script.replace('\\n ', '\\n')\n",
    "    tv_script = tv_script.replace('( ', '(')\n",
    "        \n",
    "    print(tv_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
